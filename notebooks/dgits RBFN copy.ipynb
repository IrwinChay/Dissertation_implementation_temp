{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and evaluate a PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T14:36:00.305482Z",
     "start_time": "2024-03-14T14:35:59.231534Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T14:36:01.996183Z",
     "start_time": "2024-03-14T14:36:01.950399Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")  # The device to use, e.g., \"cpu\", \"cuda\", \"cuda:1\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T14:36:03.245505Z",
     "start_time": "2024-03-14T14:36:03.190582Z"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the random seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T14:36:05.899377Z",
     "start_time": "2024-03-14T14:36:05.858609Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f88204a78b0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(4)\n",
    "np.random.seed(4)\n",
    "torch.manual_seed(4)\n",
    "# if 'cuda' in device.type:\n",
    "#     torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the training and test splits of MNIST, and preprocess them by flattening the tensor images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T14:36:10.329915Z",
     "start_time": "2024-03-14T14:36:09.956962Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: (255 * x.view(-1)).long())\n",
    "])\n",
    "data_train = datasets.MNIST('datasets', train=True, download=True, transform=transform)\n",
    "data_test = datasets.MNIST('datasets', train=False, download=True, transform=transform)\n",
    "num_variables = data_train[0][0].shape[0]\n",
    "height, width = 28, 28\n",
    "print(f\"Number of variables: {num_variables}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T12:41:27.661486Z",
     "start_time": "2024-03-14T12:41:27.534444Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.matshow(data_train[0][0].reshape(28, 28), cmap='gray')\n",
    "plt.title(f\"Class: {data_train[0][1]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating the region graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a _Quad Graph_ region graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T14:36:19.883137Z",
     "start_time": "2024-03-14T14:36:19.844640Z"
    }
   },
   "outputs": [],
   "source": [
    "from cirkit.region_graph.quad_tree import QuadTree\n",
    "# region_graph = QuadTree(width, height, struct_decomp=False)\n",
    "# region_graph = RandomBinaryTree(num_vars=128, depth=6, num_repetitions=1)\n",
    "region_graph = FullyFactorized(num_vars=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T13:01:32.902658Z",
     "start_time": "2024-03-14T13:01:32.896040Z"
    }
   },
   "outputs": [],
   "source": [
    "region_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T13:05:52.630911Z",
     "start_time": "2024-03-14T13:05:52.622639Z"
    }
   },
   "outputs": [],
   "source": [
    "region_graph._nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Others available region graphs are _Poon Domingos_ and _QuadTree_, whose imports are showed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T14:36:18.569812Z",
     "start_time": "2024-03-14T14:36:18.526134Z"
    }
   },
   "outputs": [],
   "source": [
    "from cirkit.region_graph.poon_domingos import PoonDomingos\n",
    "from cirkit.region_graph.random_binary_tree import RandomBinaryTree\n",
    "from cirkit.region_graph.fully_factorized import FullyFactorized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to choose both the input and inner layers of our circuit. As input layer we select the _CategoricalLayer_ with 256 categories (the number of pixel values). For the inner layer instead, we choose the _uncollapsed CP_ layer with rank 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T14:36:22.044953Z",
     "start_time": "2024-03-14T14:36:21.997532Z"
    }
   },
   "outputs": [],
   "source": [
    "from cirkit.layers.input.exp_family import CategoricalLayer\n",
    "from cirkit.layers.sum_product import CPLayer\n",
    "from cirkit.layers.sum_product.cp_w_bias import CPLayerWithBias\n",
    "from cirkit.layers.input.rbf_network_kernel import RBFNetworkKernelLayer\n",
    "\n",
    "efamily_cls = RBFNetworkKernelLayer\n",
    "efamily_kwargs = {}\n",
    "layer_cls = CPLayerWithBias\n",
    "layer_kwargs = {'rank': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the tensorized PC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now build our tensorized PC by specifying the region graph and layers we chose previously. In addition, we can scale the architecture by increasing the number of input and inner units. We can also have circuits with multiple output units by choosing _num_classes > 1_. However, in this notebook we only estimate the distribution of the images and marginalize out the class variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure weights are non-negative we reparametrize them via exponentiation. Several reparametrization functions are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T14:36:23.646351Z",
     "start_time": "2024-03-14T14:36:23.593945Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorizedPC(\n",
      "  (input_layer): RBFNetworkKernelLayer(\n",
      "    (params_sigma): ReparamExp()\n",
      "    (params_mu): ReparamIdentity()\n",
      "    (params_weight): ReparamIdentity()\n",
      "  )\n",
      "  (scope_layer): ScopeLayer()\n",
      "  (inner_layers): ModuleList(\n",
      "    (0): CollapsedCPLayer(\n",
      "      (params_in): ReparamLogSoftmax()\n",
      "      (params_bias): ReparamLogSoftmax()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from cirkit.reparams.leaf import ReparamExp, ReparamLogSoftmax, ReparamSoftmax, ReparamIdentity\n",
    "from cirkit.models.tensorized_circuit import TensorizedPC\n",
    "pc_rbfn = TensorizedPC.from_region_graph(\n",
    "    region_graph,\n",
    "    num_inner_units=10,\n",
    "    num_input_units=10,\n",
    "    efamily_cls=efamily_cls,\n",
    "    efamily_kwargs=efamily_kwargs,\n",
    "    layer_cls=layer_cls,\n",
    "    layer_kwargs=layer_kwargs,\n",
    "    num_classes=1,\n",
    "    reparam=ReparamLogSoftmax # ReparamExp ReparamIdentity # \n",
    ")\n",
    "pc_rbfn.to(device)\n",
    "print(pc_rbfn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be79dbb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 10])\n",
      "torch.Size([8, 10])\n",
      "torch.Size([8, 10, 10])\n",
      "torch.Size([1, 10, 1])\n",
      "torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "for param in pc_rbfn.parameters(): \n",
    "    print (param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b400ac41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cirkit.models.gp import CircuitGP, initial_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4b879cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "from uci_datasets import Dataset\n",
    "\n",
    "from ignite.engine import Events, Engine\n",
    "from ignite.metrics import Average, Loss\n",
    "from ignite.contrib.handlers import ProgressBar\n",
    "\n",
    "import gpytorch\n",
    "from gpytorch.mlls import VariationalELBO\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "baaefe59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "protein dataset, N=45730, d=9\n"
     ]
    }
   ],
   "source": [
    "data = Dataset(\"protein\")\n",
    "x_train, y_train, x_test, y_test = data.get_split(split=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8905bdc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((41157, 9), (4573, 9))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42d1accf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_real = x_train[:36584] #32000 # 2053   36584    36584     39063   13281    2672   # RE-RUN # 13279   # 1279   4701  824\n",
    "y_train_real = y_train[:36584]\n",
    "y_train_real = y_train_real.squeeze()\n",
    "x_val = x_train[36584:]\n",
    "y_val = y_train[36584:]\n",
    "y_val = y_val.squeeze()\n",
    "y_test = y_test.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ce2c6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = x_train_real.mean(axis=0)\n",
    "std = x_train_real.std(axis=0)\n",
    "\n",
    "x_train_real_normalized = (x_train_real - mean) / std\n",
    "x_val_normalized = (x_val - mean) / std\n",
    "x_test_normalized = (x_test - mean) / std\n",
    "\n",
    "# x_train_real_normalized = x_train_real\n",
    "# x_val_normalized = x_val\n",
    "# x_test_normalized = x_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cc07c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_real.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "41ddfd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(24)\n",
    "torch.manual_seed(24) ####################### CHANGE\n",
    "\n",
    "batch_size = 32 # 64\n",
    "\n",
    "ds_train = torch.utils.data.TensorDataset(torch.from_numpy(x_train_real_normalized).float(), torch.from_numpy(y_train_real).float())\n",
    "dl_train = torch.utils.data.DataLoader(ds_train, batch_size=batch_size, shuffle=True, drop_last=True) # suffle \n",
    "\n",
    "ds_val = torch.utils.data.TensorDataset(torch.from_numpy(x_val_normalized).float(), torch.from_numpy(y_val).float())\n",
    "dl_val = torch.utils.data.DataLoader(ds_val, batch_size=32, shuffle=False)\n",
    "\n",
    "ds_test = torch.utils.data.TensorDataset(torch.from_numpy(x_test_normalized).float(), torch.from_numpy(y_test).float())\n",
    "dl_test = torch.utils.data.DataLoader(ds_test, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24a3f7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorizedPC(\n",
      "  (input_layer): RBFNetworkKernelLayer(\n",
      "    (params_sigma): ReparamExp()\n",
      "    (params_mu): ReparamIdentity()\n",
      "    (params_weight): ReparamIdentity()\n",
      "  )\n",
      "  (scope_layer): ScopeLayer()\n",
      "  (inner_layers): ModuleList(\n",
      "    (0-2): 3 x CollapsedCPLayer(\n",
      "      (params_in): ReparamIdentity()\n",
      "      (params_bias): ReparamIdentity()\n",
      "    )\n",
      "    (3): SumLayer(\n",
      "      (params): ReparamIdentity()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from cirkit.region_graph.poon_domingos import PoonDomingos\n",
    "from cirkit.region_graph.random_binary_tree import RandomBinaryTree\n",
    "from cirkit.region_graph.fully_factorized import FullyFactorized\n",
    "from cirkit.region_graph.quad_tree import QuadTree\n",
    "# region_graph = QuadTree(width, height, struct_decomp=False)\n",
    "region_graph = RandomBinaryTree(num_vars=9, depth=3, num_repetitions=10)\n",
    "# region_graph = FullyFactorized(num_vars=8)\n",
    "\n",
    "\n",
    "from cirkit.layers.input.exp_family import CategoricalLayer\n",
    "from cirkit.layers.sum_product import CPLayer\n",
    "from cirkit.layers.sum_product.cp_w_bias import CPLayerWithBias\n",
    "from cirkit.layers.input.rbf_network_kernel import RBFNetworkKernelLayer\n",
    "\n",
    "efamily_cls = RBFNetworkKernelLayer\n",
    "efamily_kwargs = {}\n",
    "layer_cls = CPLayerWithBias\n",
    "layer_kwargs = {'rank': 1}\n",
    "\n",
    "num_units = 16 # 356\n",
    "\n",
    "\n",
    "from cirkit.reparams.leaf import ReparamExp, ReparamLogSoftmax, ReparamSoftmax, ReparamIdentity\n",
    "from cirkit.models.tensorized_circuit import TensorizedPC\n",
    "pc_rbfn = TensorizedPC.from_region_graph(\n",
    "    region_graph,\n",
    "    num_inner_units=num_units,\n",
    "    num_input_units=num_units,\n",
    "    efamily_cls=efamily_cls,\n",
    "    efamily_kwargs=efamily_kwargs,\n",
    "    layer_cls=layer_cls,\n",
    "    layer_kwargs=layer_kwargs,\n",
    "    num_classes=1,\n",
    "    reparam=ReparamIdentity # ReparamExp  # ReparamLogSoftmax\n",
    ")\n",
    "pc_rbfn.to(device)\n",
    "print(pc_rbfn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a16201bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 16])\n",
      "torch.Size([9, 16])\n",
      "torch.Size([9, 16, 16])\n",
      "torch.Size([40, 16, 16])\n",
      "torch.Size([40, 16])\n",
      "torch.Size([20, 16, 16])\n",
      "torch.Size([20, 16])\n",
      "torch.Size([10, 16, 1])\n",
      "torch.Size([10, 1])\n",
      "torch.Size([1, 10, 1])\n"
     ]
    }
   ],
   "source": [
    "for param in pc_rbfn.parameters(): \n",
    "    print (param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8cc84884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19092"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in pc_rbfn.parameters() if p.requires_grad)\n",
    "total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825acc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "\n",
    "# class IdentityMapping(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(IdentityMapping, self).__init__()\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         return x\n",
    "    \n",
    "# feature_extractor = IdentityMapping()\n",
    "\n",
    "# initial_centers, initial_lengthscale = initial_values(\n",
    "#     ds_train, feature_extractor, n_inducing_points=num_units\n",
    "# )\n",
    "# initial_centers.transpose(0, 1), initial_lengthscale\n",
    "\n",
    "# pc_rbfn.input_layer.params_sigma.param = torch.nn.Parameter(\n",
    "#             torch.log(initial_lengthscale * torch.ones_like(pc_rbfn.input_layer.params_sigma.param) ))\n",
    "\n",
    "# pc_rbfn.input_layer.params_mu.param = torch.nn.Parameter(initial_centers.transpose(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0dff8384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.4381, Val Loss: 0.3907\n",
      "Epoch 2: Train Loss: 0.3707, Val Loss: 0.3565\n",
      "Epoch 3: Train Loss: 0.3457, Val Loss: 0.3386\n",
      "Epoch 4: Train Loss: 0.3289, Val Loss: 0.3233\n",
      "Epoch 5: Train Loss: 0.3162, Val Loss: 0.3126\n",
      "Epoch 6: Train Loss: 0.3048, Val Loss: 0.2989\n",
      "Epoch 7: Train Loss: 0.2958, Val Loss: 0.3009\n",
      "Epoch 8: Train Loss: 0.2881, Val Loss: 0.2886\n",
      "Epoch 9: Train Loss: 0.2814, Val Loss: 0.2829\n",
      "Epoch 10: Train Loss: 0.2756, Val Loss: 0.2759\n",
      "Epoch 11: Train Loss: 0.2714, Val Loss: 0.2770\n",
      "Epoch 12: Train Loss: 0.2664, Val Loss: 0.2663\n",
      "Epoch 13: Train Loss: 0.2622, Val Loss: 0.2657\n",
      "Epoch 14: Train Loss: 0.2585, Val Loss: 0.2640\n",
      "Epoch 15: Train Loss: 0.2548, Val Loss: 0.2640\n",
      "Epoch 16: Train Loss: 0.2510, Val Loss: 0.2569\n",
      "Epoch 17: Train Loss: 0.2475, Val Loss: 0.2470\n",
      "Epoch 18: Train Loss: 0.2446, Val Loss: 0.2593\n",
      "Epoch 19: Train Loss: 0.2401, Val Loss: 0.2421\n",
      "Epoch 20: Train Loss: 0.2366, Val Loss: 0.2421\n",
      "Epoch 21: Train Loss: 0.2343, Val Loss: 0.2410\n",
      "Epoch 22: Train Loss: 0.2310, Val Loss: 0.2366\n",
      "Epoch 23: Train Loss: 0.2285, Val Loss: 0.2364\n",
      "Epoch 24: Train Loss: 0.2260, Val Loss: 0.2367\n",
      "Epoch 25: Train Loss: 0.2234, Val Loss: 0.2351\n",
      "Epoch 26: Train Loss: 0.2206, Val Loss: 0.2352\n",
      "Epoch 27: Train Loss: 0.2174, Val Loss: 0.2320\n",
      "Epoch 28: Train Loss: 0.2155, Val Loss: 0.2296\n",
      "Epoch 29: Train Loss: 0.2142, Val Loss: 0.2260\n",
      "Epoch 30: Train Loss: 0.2113, Val Loss: 0.2237\n",
      "Epoch 31: Train Loss: 0.2092, Val Loss: 0.2289\n",
      "Epoch 32: Train Loss: 0.2065, Val Loss: 0.2191\n",
      "Epoch 33: Train Loss: 0.2052, Val Loss: 0.2181\n",
      "Epoch 34: Train Loss: 0.2030, Val Loss: 0.2188\n",
      "Epoch 35: Train Loss: 0.2013, Val Loss: 0.2206\n",
      "Epoch 36: Train Loss: 0.1993, Val Loss: 0.2136\n",
      "Epoch 37: Train Loss: 0.1968, Val Loss: 0.2161\n",
      "Epoch 38: Train Loss: 0.1956, Val Loss: 0.2149\n",
      "Epoch 39: Train Loss: 0.1933, Val Loss: 0.2152\n",
      "Epoch 40: Train Loss: 0.1913, Val Loss: 0.2154\n",
      "Epoch 41: Train Loss: 0.1897, Val Loss: 0.2113\n",
      "Epoch 42: Train Loss: 0.1881, Val Loss: 0.2109\n",
      "Epoch 43: Train Loss: 0.1863, Val Loss: 0.2080\n",
      "Epoch 44: Train Loss: 0.1845, Val Loss: 0.2104\n",
      "Epoch 45: Train Loss: 0.1828, Val Loss: 0.2106\n",
      "Epoch 46: Train Loss: 0.1816, Val Loss: 0.2146\n",
      "Epoch 47: Train Loss: 0.1802, Val Loss: 0.2100\n",
      "Epoch 48: Train Loss: 0.1782, Val Loss: 0.2151\n",
      "Epoch 49: Train Loss: 0.1771, Val Loss: 0.2100\n",
      "Epoch 50: Train Loss: 0.1753, Val Loss: 0.2110\n",
      "Epoch 51: Train Loss: 0.1745, Val Loss: 0.2182\n",
      "Epoch 52: Train Loss: 0.1737, Val Loss: 0.2122\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(24)\n",
    "np.random.seed(24)\n",
    "\n",
    "# Parameters\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Assume pc_rbfn is your model, already defined and initialized elsewhere\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(pc_rbfn.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "nan_counter = 0\n",
    "exit_loops = False\n",
    "\n",
    "# Training and Validation Loop\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    pc_rbfn.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for inputs, targets in dl_train:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = pc_rbfn(inputs)\n",
    "        outputs = outputs.squeeze(1)  # Ensure outputs match the target's shape\n",
    "        \n",
    "        # if(torch.isnan(outputs).any() == False):\n",
    "        #     print(\"no NAN\")\n",
    "\n",
    "        if(torch.isnan(outputs).any()):\n",
    "            print(\"outputs\", outputs)\n",
    "        \n",
    "        if(torch.isnan(targets).any()):\n",
    "            print(\"targets\", targets)\n",
    "        \n",
    "        loss = criterion(outputs, targets)\n",
    "        if(torch.isnan(loss).any()):\n",
    "            print(\"loss\", loss)\n",
    "            nan_counter += 1\n",
    "        if (nan_counter > 3):\n",
    "            exit_loops = True\n",
    "            break\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "    if exit_loops:\n",
    "        break\n",
    "    train_loss /= len(dl_train.dataset)\n",
    "\n",
    "    # Validation phase\n",
    "    pc_rbfn.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dl_val:\n",
    "            outputs = pc_rbfn(inputs)\n",
    "            outputs = outputs.squeeze(1)  # Ensure outputs match the target's shape\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "    val_loss /= len(dl_val.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3a338f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "test_loss = 0.0\n",
    "pc_rbfn.eval()  # Ensure model is in evaluation mode\n",
    "with torch.no_grad():  # No gradients needed\n",
    "    for inputs, targets in dl_test:\n",
    "        outputs = pc_rbfn(inputs).squeeze(1)\n",
    "        loss = criterion(outputs, targets)\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "test_loss /= len(dl_test.dataset)\n",
    "rmse = np.sqrt(test_loss)  # Calculate RMSE\n",
    "print(f\"Test RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dc6af2",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dce04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# Define the transformation to normalize the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.view(-1))\n",
    "])\n",
    "\n",
    "# Load MNIST dataset\n",
    "train_val_dataset = datasets.MNIST('datasets', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('datasets', train=False, download=True, transform=transform)\n",
    "\n",
    "# Split train dataset into train and validation\n",
    "train_size = int(len(train_val_dataset) * 0.9)\n",
    "val_size = len(train_val_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_val_dataset, [train_size, val_size])\n",
    "\n",
    "# DataLoader\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a672f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    print(f\"Batch index: {batch_idx}\")\n",
    "    print(f\"Input tensor shape: {data.shape}\")  # Shape will be [batch_size, channels, height, width] for images\n",
    "    print(f\"Target tensor shape: {target.shape}\")  # Shape will be [batch_size] for the labels\n",
    "    # Optionally break after the first batch to just see one example\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b72130",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cirkit.region_graph.poon_domingos import PoonDomingos\n",
    "from cirkit.region_graph.random_binary_tree import RandomBinaryTree\n",
    "from cirkit.region_graph.fully_factorized import FullyFactorized\n",
    "from cirkit.region_graph.quad_tree import QuadTree\n",
    "region_graph = QuadTree(28, 28, struct_decomp=True)\n",
    "# region_graph = RandomBinaryTree(num_vars=8, depth=3, num_repetitions=1)\n",
    "# region_graph = FullyFactorized(num_vars=8)\n",
    "\n",
    "\n",
    "from cirkit.layers.input.exp_family import CategoricalLayer\n",
    "from cirkit.layers.sum_product import CPLayer\n",
    "from cirkit.layers.sum_product.cp_w_bias import CPLayerWithBias\n",
    "from cirkit.layers.input.rbf_network_kernel import RBFNetworkKernelLayer\n",
    "\n",
    "efamily_cls = RBFNetworkKernelLayer\n",
    "efamily_kwargs = {}\n",
    "layer_cls = CPLayerWithBias\n",
    "layer_kwargs = {'rank': 1}\n",
    "\n",
    "num_units = 32 # 356\n",
    "\n",
    "\n",
    "from cirkit.reparams.leaf import ReparamExp, ReparamLogSoftmax, ReparamSoftmax, ReparamIdentity\n",
    "from cirkit.models.tensorized_circuit import TensorizedPC\n",
    "pc_rbfn = TensorizedPC.from_region_graph(\n",
    "    region_graph,\n",
    "    num_inner_units=num_units,\n",
    "    num_input_units=num_units,\n",
    "    efamily_cls=efamily_cls,\n",
    "    efamily_kwargs=efamily_kwargs,\n",
    "    layer_cls=layer_cls,\n",
    "    layer_kwargs=layer_kwargs,\n",
    "    num_classes=10,\n",
    "    reparam=ReparamIdentity # ReparamExp  # ReparamLogSoftmax\n",
    ")\n",
    "pc_rbfn.to(device)\n",
    "print(pc_rbfn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73612a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in pc_rbfn.parameters():\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d2c61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model, loss function, and optimizer\n",
    "model = pc_rbfn\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training and validation loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "# Add your test loop here if needed, following the same pattern as validation\n",
    "# Remember to evaluate the model's performance on the test set after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a60fcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e01928c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea040f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f1bf0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ff88bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8772bf53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335c7855",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3c0626",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aef14dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee87253",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a6f8e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa77efd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef03dad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5028f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cirkit1",
   "language": "python",
   "name": "cirkit1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
