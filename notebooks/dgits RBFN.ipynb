{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and evaluate a PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T14:36:00.305482Z",
     "start_time": "2024-03-14T14:35:59.231534Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T14:36:01.996183Z",
     "start_time": "2024-03-14T14:36:01.950399Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")  # The device to use, e.g., \"cpu\", \"cuda\", \"cuda:1\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T14:36:03.245505Z",
     "start_time": "2024-03-14T14:36:03.190582Z"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the random seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T14:36:05.899377Z",
     "start_time": "2024-03-14T14:36:05.858609Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f7c68cf4890>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(4)\n",
    "np.random.seed(4)\n",
    "torch.manual_seed(4)\n",
    "# if 'cuda' in device.type:\n",
    "#     torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the training and test splits of MNIST, and preprocess them by flattening the tensor images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T14:36:10.329915Z",
     "start_time": "2024-03-14T14:36:09.956962Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: (255 * x.view(-1)).long())\n",
    "])\n",
    "data_train = datasets.MNIST('datasets', train=True, download=True, transform=transform)\n",
    "data_test = datasets.MNIST('datasets', train=False, download=True, transform=transform)\n",
    "num_variables = data_train[0][0].shape[0]\n",
    "height, width = 28, 28\n",
    "print(f\"Number of variables: {num_variables}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T12:41:27.661486Z",
     "start_time": "2024-03-14T12:41:27.534444Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.matshow(data_train[0][0].reshape(28, 28), cmap='gray')\n",
    "plt.title(f\"Class: {data_train[0][1]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating the region graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a _Quad Graph_ region graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T14:36:19.883137Z",
     "start_time": "2024-03-14T14:36:19.844640Z"
    }
   },
   "outputs": [],
   "source": [
    "from cirkit.region_graph.quad_tree import QuadTree\n",
    "# region_graph = QuadTree(width, height, struct_decomp=False)\n",
    "# region_graph = RandomBinaryTree(num_vars=128, depth=6, num_repetitions=1)\n",
    "region_graph = FullyFactorized(num_vars=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T13:01:32.902658Z",
     "start_time": "2024-03-14T13:01:32.896040Z"
    }
   },
   "outputs": [],
   "source": [
    "region_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T13:05:52.630911Z",
     "start_time": "2024-03-14T13:05:52.622639Z"
    }
   },
   "outputs": [],
   "source": [
    "region_graph._nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Others available region graphs are _Poon Domingos_ and _QuadTree_, whose imports are showed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T14:36:18.569812Z",
     "start_time": "2024-03-14T14:36:18.526134Z"
    }
   },
   "outputs": [],
   "source": [
    "from cirkit.region_graph.poon_domingos import PoonDomingos\n",
    "from cirkit.region_graph.random_binary_tree import RandomBinaryTree\n",
    "from cirkit.region_graph.fully_factorized import FullyFactorized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to choose both the input and inner layers of our circuit. As input layer we select the _CategoricalLayer_ with 256 categories (the number of pixel values). For the inner layer instead, we choose the _uncollapsed CP_ layer with rank 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T14:36:22.044953Z",
     "start_time": "2024-03-14T14:36:21.997532Z"
    }
   },
   "outputs": [],
   "source": [
    "from cirkit.layers.input.exp_family import CategoricalLayer\n",
    "from cirkit.layers.sum_product import CPLayer\n",
    "from cirkit.layers.sum_product.cp_w_bias import CPLayerWithBias\n",
    "from cirkit.layers.input.rbf_network_kernel import RBFNetworkKernelLayer\n",
    "\n",
    "efamily_cls = RBFNetworkKernelLayer\n",
    "efamily_kwargs = {}\n",
    "layer_cls = CPLayerWithBias\n",
    "layer_kwargs = {'rank': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the tensorized PC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now build our tensorized PC by specifying the region graph and layers we chose previously. In addition, we can scale the architecture by increasing the number of input and inner units. We can also have circuits with multiple output units by choosing _num_classes > 1_. However, in this notebook we only estimate the distribution of the images and marginalize out the class variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure weights are non-negative we reparametrize them via exponentiation. Several reparametrization functions are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T14:36:23.646351Z",
     "start_time": "2024-03-14T14:36:23.593945Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorizedPC(\n",
      "  (input_layer): RBFNetworkKernelLayer(\n",
      "    (params_sigma): ReparamExp()\n",
      "    (params_mu): ReparamIdentity()\n",
      "  )\n",
      "  (scope_layer): ScopeLayer()\n",
      "  (inner_layers): ModuleList(\n",
      "    (0): CollapsedCPLayer(\n",
      "      (params_in): ReparamLogSoftmax()\n",
      "      (params_bias): ReparamLogSoftmax()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from cirkit.reparams.leaf import ReparamExp, ReparamLogSoftmax, ReparamSoftmax, ReparamIdentity\n",
    "from cirkit.models.tensorized_circuit import TensorizedPC\n",
    "pc_rbfn = TensorizedPC.from_region_graph(\n",
    "    region_graph,\n",
    "    num_inner_units=10,\n",
    "    num_input_units=10,\n",
    "    efamily_cls=efamily_cls,\n",
    "    efamily_kwargs=efamily_kwargs,\n",
    "    layer_cls=layer_cls,\n",
    "    layer_kwargs=layer_kwargs,\n",
    "    num_classes=1,\n",
    "    reparam=ReparamLogSoftmax # ReparamExp ReparamIdentity # \n",
    ")\n",
    "pc_rbfn.to(device)\n",
    "print(pc_rbfn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "be79dbb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 10])\n",
      "torch.Size([8, 10])\n",
      "torch.Size([1, 8, 10, 1])\n",
      "torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "for param in pc_rbfn.parameters(): \n",
    "    print (param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b400ac41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cirkit.models.gp import CircuitGP, initial_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e4b879cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "from uci_datasets import Dataset\n",
    "\n",
    "from ignite.engine import Events, Engine\n",
    "from ignite.metrics import Average, Loss\n",
    "from ignite.contrib.handlers import ProgressBar\n",
    "\n",
    "import gpytorch\n",
    "from gpytorch.mlls import VariationalELBO\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "baaefe59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "protein dataset, N=45730, d=9\n"
     ]
    }
   ],
   "source": [
    "data = Dataset(\"protein\")\n",
    "x_train, y_train, x_test, y_test = data.get_split(split=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "8905bdc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((41157, 9), (4573, 9))"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "42d1accf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_real = x_train[:36584] #32000 # 2053   36584    36584     39063   13281    2672   # RE-RUN # 13279   # 1279   4701  824\n",
    "y_train_real = y_train[:36584]\n",
    "y_train_real = y_train_real.squeeze()\n",
    "x_val = x_train[36584:]\n",
    "y_val = y_train[36584:]\n",
    "y_val = y_val.squeeze()\n",
    "y_test = y_test.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "0ce2c6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = x_train_real.mean(axis=0)\n",
    "std = x_train_real.std(axis=0)\n",
    "\n",
    "x_train_real_normalized = (x_train_real - mean) / std\n",
    "x_val_normalized = (x_val - mean) / std\n",
    "x_test_normalized = (x_test - mean) / std\n",
    "\n",
    "# x_train_real_normalized = x_train_real\n",
    "# x_val_normalized = x_val\n",
    "# x_test_normalized = x_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "84cc07c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_real.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "41ddfd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(24)\n",
    "torch.manual_seed(24) ####################### CHANGE\n",
    "\n",
    "batch_size = 32 # 64\n",
    "\n",
    "ds_train = torch.utils.data.TensorDataset(torch.from_numpy(x_train_real_normalized).float(), torch.from_numpy(y_train_real).float())\n",
    "dl_train = torch.utils.data.DataLoader(ds_train, batch_size=batch_size, shuffle=True, drop_last=True) # suffle \n",
    "\n",
    "ds_val = torch.utils.data.TensorDataset(torch.from_numpy(x_val_normalized).float(), torch.from_numpy(y_val).float())\n",
    "dl_val = torch.utils.data.DataLoader(ds_val, batch_size=512, shuffle=False)\n",
    "\n",
    "ds_test = torch.utils.data.TensorDataset(torch.from_numpy(x_test_normalized).float(), torch.from_numpy(y_test).float())\n",
    "dl_test = torch.utils.data.DataLoader(ds_test, batch_size=512, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "24a3f7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorizedPC(\n",
      "  (input_layer): RBFNetworkKernelLayer(\n",
      "    (params_sigma): ReparamExp()\n",
      "    (params_mu): ReparamIdentity()\n",
      "    (params_weight): ReparamIdentity()\n",
      "  )\n",
      "  (scope_layer): ScopeLayer()\n",
      "  (inner_layers): ModuleList(\n",
      "    (0-2): 3 x CollapsedCPLayer(\n",
      "      (params_in): ReparamIdentity()\n",
      "      (params_bias): ReparamIdentity()\n",
      "    )\n",
      "    (3): SumLayer(\n",
      "      (params): ReparamIdentity()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from cirkit.region_graph.poon_domingos import PoonDomingos\n",
    "from cirkit.region_graph.random_binary_tree import RandomBinaryTree\n",
    "from cirkit.region_graph.fully_factorized import FullyFactorized\n",
    "from cirkit.region_graph.quad_tree import QuadTree\n",
    "# region_graph = QuadTree(width, height, struct_decomp=False)\n",
    "region_graph = RandomBinaryTree(num_vars=9, depth=3, num_repetitions=10)\n",
    "# region_graph = FullyFactorized(num_vars=8)\n",
    "\n",
    "\n",
    "from cirkit.layers.input.exp_family import CategoricalLayer\n",
    "from cirkit.layers.sum_product import CPLayer\n",
    "from cirkit.layers.sum_product.cp_w_bias import CPLayerWithBias\n",
    "from cirkit.layers.input.rbf_network_kernel import RBFNetworkKernelLayer\n",
    "\n",
    "efamily_cls = RBFNetworkKernelLayer\n",
    "efamily_kwargs = {}\n",
    "layer_cls = CPLayerWithBias\n",
    "layer_kwargs = {'rank': 1}\n",
    "\n",
    "num_units = 16 # 356\n",
    "\n",
    "\n",
    "from cirkit.reparams.leaf import ReparamExp, ReparamLogSoftmax, ReparamSoftmax, ReparamIdentity\n",
    "from cirkit.models.tensorized_circuit import TensorizedPC\n",
    "pc_rbfn = TensorizedPC.from_region_graph(\n",
    "    region_graph,\n",
    "    num_inner_units=num_units,\n",
    "    num_input_units=num_units,\n",
    "    efamily_cls=efamily_cls,\n",
    "    efamily_kwargs=efamily_kwargs,\n",
    "    layer_cls=layer_cls,\n",
    "    layer_kwargs=layer_kwargs,\n",
    "    num_classes=1,\n",
    "    reparam=ReparamIdentity # ReparamExp  # ReparamLogSoftmax\n",
    ")\n",
    "pc_rbfn.to(device)\n",
    "print(pc_rbfn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "a16201bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 16])\n",
      "torch.Size([9, 16])\n",
      "torch.Size([9, 16, 16])\n",
      "torch.Size([40, 16, 16])\n",
      "torch.Size([40, 16])\n",
      "torch.Size([20, 16, 16])\n",
      "torch.Size([20, 16])\n",
      "torch.Size([10, 16, 1])\n",
      "torch.Size([10, 1])\n",
      "torch.Size([1, 10, 1])\n"
     ]
    }
   ],
   "source": [
    "for param in pc_rbfn.parameters(): \n",
    "    print (param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "8cc84884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19092"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in pc_rbfn.parameters() if p.requires_grad)\n",
    "total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "825acc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_X_samples torch.Size([13281, 17])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/irwinchay/opt/anaconda3/envs/cirkit1/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_lengthscale tensor(5.6649)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class IdentityMapping(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IdentityMapping, self).__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "feature_extractor = IdentityMapping()\n",
    "\n",
    "initial_centers, initial_lengthscale = initial_values(\n",
    "    ds_train, feature_extractor, n_inducing_points=num_units\n",
    ")\n",
    "initial_centers.transpose(0, 1), initial_lengthscale\n",
    "\n",
    "pc_rbfn.input_layer.params_sigma.param = torch.nn.Parameter(\n",
    "            torch.log(initial_lengthscale * torch.ones_like(pc_rbfn.input_layer.params_sigma.param) ))\n",
    "\n",
    "pc_rbfn.input_layer.params_mu.param = torch.nn.Parameter(initial_centers.transpose(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "0dff8384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 15.1936, Val Loss: 0.8443\n",
      "Epoch 2: Train Loss: 0.6106, Val Loss: 0.4992\n",
      "Epoch 3: Train Loss: 0.4488, Val Loss: 0.4333\n",
      "Epoch 4: Train Loss: 0.4062, Val Loss: 0.3975\n",
      "Epoch 5: Train Loss: 0.3873, Val Loss: 0.3757\n",
      "Epoch 6: Train Loss: 0.3734, Val Loss: 0.3630\n",
      "Epoch 7: Train Loss: 0.3628, Val Loss: 0.3488\n",
      "Epoch 8: Train Loss: 0.3528, Val Loss: 0.3532\n",
      "Epoch 9: Train Loss: 0.3414, Val Loss: 0.3344\n",
      "Epoch 10: Train Loss: 0.3342, Val Loss: 0.3429\n",
      "Epoch 11: Train Loss: 0.3288, Val Loss: 0.3268\n",
      "Epoch 12: Train Loss: 0.3208, Val Loss: 0.3138\n",
      "Epoch 13: Train Loss: 0.3151, Val Loss: 0.3046\n",
      "Epoch 14: Train Loss: 0.3085, Val Loss: 0.3084\n",
      "Epoch 15: Train Loss: 0.3031, Val Loss: 0.3074\n",
      "Epoch 16: Train Loss: 0.2975, Val Loss: 0.3113\n",
      "Epoch 17: Train Loss: 0.2935, Val Loss: 0.2820\n",
      "Epoch 18: Train Loss: 0.2883, Val Loss: 0.2999\n",
      "Epoch 19: Train Loss: 0.2828, Val Loss: 0.2792\n",
      "Epoch 20: Train Loss: 0.2789, Val Loss: 0.2795\n",
      "Epoch 21: Train Loss: 0.2754, Val Loss: 0.2805\n",
      "Epoch 22: Train Loss: 0.2702, Val Loss: 0.2687\n",
      "Epoch 23: Train Loss: 0.2669, Val Loss: 0.2600\n",
      "Epoch 24: Train Loss: 0.2630, Val Loss: 0.2604\n",
      "Epoch 25: Train Loss: 0.2601, Val Loss: 0.2572\n",
      "Epoch 26: Train Loss: 0.2554, Val Loss: 0.2513\n",
      "Epoch 27: Train Loss: 0.2510, Val Loss: 0.2497\n",
      "Epoch 28: Train Loss: 0.2473, Val Loss: 0.2533\n",
      "Epoch 29: Train Loss: 0.2454, Val Loss: 0.2419\n",
      "Epoch 30: Train Loss: 0.2413, Val Loss: 0.2471\n",
      "Epoch 31: Train Loss: 0.2380, Val Loss: 0.2409\n",
      "Epoch 32: Train Loss: 0.2342, Val Loss: 0.2350\n",
      "Epoch 33: Train Loss: 0.2318, Val Loss: 0.2307\n",
      "Epoch 34: Train Loss: 0.2293, Val Loss: 0.2332\n",
      "Epoch 35: Train Loss: 0.2268, Val Loss: 0.2371\n",
      "Epoch 36: Train Loss: 0.2238, Val Loss: 0.2339\n",
      "Epoch 37: Train Loss: 0.2200, Val Loss: 0.2259\n",
      "Epoch 38: Train Loss: 0.2182, Val Loss: 0.2273\n",
      "Epoch 39: Train Loss: 0.2151, Val Loss: 0.2231\n",
      "Epoch 40: Train Loss: 0.2127, Val Loss: 0.2273\n",
      "Epoch 41: Train Loss: 0.2101, Val Loss: 0.2185\n",
      "Epoch 42: Train Loss: 0.2068, Val Loss: 0.2173\n",
      "Epoch 43: Train Loss: 0.2048, Val Loss: 0.2136\n",
      "Epoch 44: Train Loss: 0.2022, Val Loss: 0.2176\n",
      "Epoch 45: Train Loss: 0.2005, Val Loss: 0.2165\n",
      "Epoch 46: Train Loss: 0.1978, Val Loss: 0.2183\n",
      "Epoch 47: Train Loss: 0.1961, Val Loss: 0.2149\n",
      "Epoch 48: Train Loss: 0.1936, Val Loss: 0.2143\n",
      "Epoch 49: Train Loss: 0.1911, Val Loss: 0.2118\n",
      "Epoch 50: Train Loss: 0.1893, Val Loss: 0.2169\n",
      "Epoch 51: Train Loss: 0.1883, Val Loss: 0.2202\n",
      "Epoch 52: Train Loss: 0.1866, Val Loss: 0.2109\n",
      "Epoch 53: Train Loss: 0.1838, Val Loss: 0.2073\n",
      "Epoch 54: Train Loss: 0.1832, Val Loss: 0.2037\n",
      "Epoch 55: Train Loss: 0.1811, Val Loss: 0.2084\n",
      "Epoch 56: Train Loss: 0.1804, Val Loss: 0.2066\n",
      "Epoch 57: Train Loss: 0.1778, Val Loss: 0.2058\n",
      "Epoch 58: Train Loss: 0.1767, Val Loss: 0.2098\n",
      "Epoch 59: Train Loss: 0.1749, Val Loss: 0.2071\n",
      "Epoch 60: Train Loss: 0.1736, Val Loss: 0.2019\n",
      "Epoch 61: Train Loss: 0.1722, Val Loss: 0.2050\n",
      "Epoch 62: Train Loss: 0.1711, Val Loss: 0.2032\n",
      "Epoch 63: Train Loss: 0.1700, Val Loss: 0.2031\n",
      "Epoch 64: Train Loss: 0.1684, Val Loss: 0.2039\n",
      "Epoch 65: Train Loss: 0.1671, Val Loss: 0.2083\n",
      "Epoch 66: Train Loss: 0.1658, Val Loss: 0.2006\n",
      "Epoch 67: Train Loss: 0.1641, Val Loss: 0.2003\n",
      "Epoch 68: Train Loss: 0.1633, Val Loss: 0.2022\n",
      "Epoch 69: Train Loss: 0.1618, Val Loss: 0.2103\n",
      "Epoch 70: Train Loss: 0.1611, Val Loss: 0.2056\n",
      "Epoch 71: Train Loss: 0.1599, Val Loss: 0.2002\n",
      "Epoch 72: Train Loss: 0.1581, Val Loss: 0.1991\n",
      "Epoch 73: Train Loss: 0.1574, Val Loss: 0.2059\n",
      "Epoch 74: Train Loss: 0.1559, Val Loss: 0.2008\n",
      "Epoch 75: Train Loss: 0.1551, Val Loss: 0.2054\n",
      "Epoch 76: Train Loss: 0.1533, Val Loss: 0.2041\n",
      "Epoch 77: Train Loss: 0.1528, Val Loss: 0.2023\n",
      "Epoch 78: Train Loss: 0.1517, Val Loss: 0.1992\n",
      "Epoch 79: Train Loss: 0.1509, Val Loss: 0.1994\n",
      "Epoch 80: Train Loss: 0.1498, Val Loss: 0.1991\n",
      "Epoch 81: Train Loss: 0.1490, Val Loss: 0.2020\n",
      "Epoch 82: Train Loss: 0.1477, Val Loss: 0.2023\n",
      "Epoch 83: Train Loss: 0.1469, Val Loss: 0.1978\n",
      "Epoch 84: Train Loss: 0.1458, Val Loss: 0.2000\n",
      "Epoch 85: Train Loss: 0.1446, Val Loss: 0.2002\n",
      "Epoch 86: Train Loss: 0.1436, Val Loss: 0.2015\n",
      "Epoch 87: Train Loss: 0.1428, Val Loss: 0.2014\n",
      "Epoch 88: Train Loss: 0.1423, Val Loss: 0.2036\n",
      "Epoch 89: Train Loss: 0.1418, Val Loss: 0.1979\n",
      "Epoch 90: Train Loss: 0.1404, Val Loss: 0.2005\n",
      "Epoch 91: Train Loss: 0.1393, Val Loss: 0.1988\n",
      "Epoch 92: Train Loss: 0.1386, Val Loss: 0.1981\n",
      "Epoch 93: Train Loss: 0.1383, Val Loss: 0.2012\n",
      "Epoch 94: Train Loss: 0.1375, Val Loss: 0.1964\n",
      "Epoch 95: Train Loss: 0.1362, Val Loss: 0.1991\n",
      "Epoch 96: Train Loss: 0.1357, Val Loss: 0.1961\n",
      "Epoch 97: Train Loss: 0.1344, Val Loss: 0.2012\n",
      "Epoch 98: Train Loss: 0.1343, Val Loss: 0.2023\n",
      "Epoch 99: Train Loss: 0.1333, Val Loss: 0.1989\n",
      "Epoch 100: Train Loss: 0.1327, Val Loss: 0.2008\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(24)\n",
    "np.random.seed(24)\n",
    "\n",
    "# Parameters\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Assume pc_rbfn is your model, already defined and initialized elsewhere\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(pc_rbfn.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "nan_counter = 0\n",
    "exit_loops = False\n",
    "\n",
    "# Training and Validation Loop\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    pc_rbfn.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for inputs, targets in dl_train:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = pc_rbfn(inputs)\n",
    "        outputs = outputs.squeeze(1)  # Ensure outputs match the target's shape\n",
    "        \n",
    "        # if(torch.isnan(outputs).any() == False):\n",
    "        #     print(\"no NAN\")\n",
    "\n",
    "        if(torch.isnan(outputs).any()):\n",
    "            print(\"outputs\", outputs)\n",
    "        \n",
    "        if(torch.isnan(targets).any()):\n",
    "            print(\"targets\", targets)\n",
    "        \n",
    "        loss = criterion(outputs, targets)\n",
    "        if(torch.isnan(loss).any()):\n",
    "            print(\"loss\", loss)\n",
    "            nan_counter += 1\n",
    "        if (nan_counter > 3):\n",
    "            exit_loops = True\n",
    "            break\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "    if exit_loops:\n",
    "        break\n",
    "    train_loss /= len(dl_train.dataset)\n",
    "\n",
    "    # Validation phase\n",
    "    pc_rbfn.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dl_val:\n",
    "            outputs = pc_rbfn(inputs)\n",
    "            outputs = outputs.squeeze(1)  # Ensure outputs match the target's shape\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "    val_loss /= len(dl_val.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "bd3a338f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.4639\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "test_loss = 0.0\n",
    "pc_rbfn.eval()  # Ensure model is in evaluation mode\n",
    "with torch.no_grad():  # No gradients needed\n",
    "    for inputs, targets in dl_test:\n",
    "        outputs = pc_rbfn(inputs).squeeze(1)\n",
    "        loss = criterion(outputs, targets)\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "test_loss /= len(dl_test.dataset)\n",
    "rmse = np.sqrt(test_loss)  # Calculate RMSE\n",
    "print(f\"Test RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dc6af2",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "67dce04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# Define the transformation to normalize the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.view(-1))\n",
    "])\n",
    "\n",
    "# Load MNIST dataset\n",
    "train_val_dataset = datasets.MNIST('datasets', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('datasets', train=False, download=True, transform=transform)\n",
    "\n",
    "# Split train dataset into train and validation\n",
    "train_size = int(len(train_val_dataset) * 0.9)\n",
    "val_size = len(train_val_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_val_dataset, [train_size, val_size])\n",
    "\n",
    "# DataLoader\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "37a672f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch index: 0\n",
      "Input tensor shape: torch.Size([32, 784])\n",
      "Target tensor shape: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    print(f\"Batch index: {batch_idx}\")\n",
    "    print(f\"Input tensor shape: {data.shape}\")  # Shape will be [batch_size, channels, height, width] for images\n",
    "    print(f\"Target tensor shape: {target.shape}\")  # Shape will be [batch_size] for the labels\n",
    "    # Optionally break after the first batch to just see one example\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "64b72130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorizedPC(\n",
      "  (input_layer): RBFNetworkKernelLayer(\n",
      "    (params_sigma): ReparamExp()\n",
      "    (params_mu): ReparamIdentity()\n",
      "    (params_weight): ReparamIdentity()\n",
      "  )\n",
      "  (scope_layer): ScopeLayer()\n",
      "  (inner_layers): ModuleList(\n",
      "    (0-9): 10 x CollapsedCPLayer(\n",
      "      (params_in): ReparamIdentity()\n",
      "      (params_bias): ReparamIdentity()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from cirkit.region_graph.poon_domingos import PoonDomingos\n",
    "from cirkit.region_graph.random_binary_tree import RandomBinaryTree\n",
    "from cirkit.region_graph.fully_factorized import FullyFactorized\n",
    "from cirkit.region_graph.quad_tree import QuadTree\n",
    "region_graph = QuadTree(28, 28, struct_decomp=True)\n",
    "# region_graph = RandomBinaryTree(num_vars=8, depth=3, num_repetitions=1)\n",
    "# region_graph = FullyFactorized(num_vars=8)\n",
    "\n",
    "\n",
    "from cirkit.layers.input.exp_family import CategoricalLayer\n",
    "from cirkit.layers.sum_product import CPLayer\n",
    "from cirkit.layers.sum_product.cp_w_bias import CPLayerWithBias\n",
    "from cirkit.layers.input.rbf_network_kernel import RBFNetworkKernelLayer\n",
    "\n",
    "efamily_cls = RBFNetworkKernelLayer\n",
    "efamily_kwargs = {}\n",
    "layer_cls = CPLayerWithBias\n",
    "layer_kwargs = {'rank': 1}\n",
    "\n",
    "num_units = 32 # 356\n",
    "\n",
    "\n",
    "from cirkit.reparams.leaf import ReparamExp, ReparamLogSoftmax, ReparamSoftmax, ReparamIdentity\n",
    "from cirkit.models.tensorized_circuit import TensorizedPC\n",
    "pc_rbfn = TensorizedPC.from_region_graph(\n",
    "    region_graph,\n",
    "    num_inner_units=num_units,\n",
    "    num_input_units=num_units,\n",
    "    efamily_cls=efamily_cls,\n",
    "    efamily_kwargs=efamily_kwargs,\n",
    "    layer_cls=layer_cls,\n",
    "    layer_kwargs=layer_kwargs,\n",
    "    num_classes=10,\n",
    "    reparam=ReparamIdentity # ReparamExp  # ReparamLogSoftmax\n",
    ")\n",
    "pc_rbfn.to(device)\n",
    "print(pc_rbfn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "73612a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([784, 32])\n",
      "torch.Size([784, 32])\n",
      "torch.Size([784, 32, 32])\n",
      "torch.Size([288, 32, 32])\n",
      "torch.Size([288, 32])\n",
      "torch.Size([240, 32, 32])\n",
      "torch.Size([240, 32])\n",
      "torch.Size([128, 32, 32])\n",
      "torch.Size([128, 32])\n",
      "torch.Size([64, 32, 32])\n",
      "torch.Size([64, 32])\n",
      "torch.Size([32, 32, 32])\n",
      "torch.Size([32, 32])\n",
      "torch.Size([16, 32, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([8, 32, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([4, 32, 32])\n",
      "torch.Size([4, 32])\n",
      "torch.Size([2, 32, 32])\n",
      "torch.Size([2, 32])\n",
      "torch.Size([1, 32, 10])\n",
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "for param in pc_rbfn.parameters():\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "05d2c61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: nan, Val Loss: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[386], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[1;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> 15\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     17\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cirkit1/lib/python3.8/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cirkit1/lib/python3.8/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Instantiate the model, loss function, and optimizer\n",
    "model = pc_rbfn\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training and validation loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "# Add your test loop here if needed, following the same pattern as validation\n",
    "# Remember to evaluate the model's performance on the test set after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a60fcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e01928c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea040f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f1bf0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ff88bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8772bf53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335c7855",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3c0626",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aef14dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee87253",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a6f8e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa77efd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef03dad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5028f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cirkit1",
   "language": "python",
   "name": "cirkit1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
