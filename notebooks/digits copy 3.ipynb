{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and evaluate a PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T14:36:00.305482Z",
     "start_time": "2024-03-14T14:35:59.231534Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T14:36:01.996183Z",
     "start_time": "2024-03-14T14:36:01.950399Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")  # The device to use, e.g., \"cpu\", \"cuda\", \"cuda:1\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T14:36:03.245505Z",
     "start_time": "2024-03-14T14:36:03.190582Z"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the random seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T14:36:05.899377Z",
     "start_time": "2024-03-14T14:36:05.858609Z"
    }
   },
   "outputs": [],
   "source": [
    "random.seed(4)\n",
    "np.random.seed(4)\n",
    "torch.manual_seed(4)\n",
    "# if 'cuda' in device.type:\n",
    "#     torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the training and test splits of MNIST, and preprocess them by flattening the tensor images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T14:36:10.329915Z",
     "start_time": "2024-03-14T14:36:09.956962Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: (255 * x.view(-1)).long())\n",
    "])\n",
    "data_train = datasets.MNIST('datasets', train=True, download=True, transform=transform)\n",
    "data_test = datasets.MNIST('datasets', train=False, download=True, transform=transform)\n",
    "num_variables = data_train[0][0].shape[0]\n",
    "height, width = 28, 28\n",
    "print(f\"Number of variables: {num_variables}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T12:41:27.661486Z",
     "start_time": "2024-03-14T12:41:27.534444Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.matshow(data_train[0][0].reshape(28, 28), cmap='gray')\n",
    "plt.title(f\"Class: {data_train[0][1]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating the region graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a _Quad Graph_ region graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T14:36:19.883137Z",
     "start_time": "2024-03-14T14:36:19.844640Z"
    }
   },
   "outputs": [],
   "source": [
    "from cirkit.region_graph.quad_tree import QuadTree\n",
    "# region_graph = QuadTree(width, height, struct_decomp=False)\n",
    "# region_graph = RandomBinaryTree(num_vars=18, depth=4, num_repetitions=1)\n",
    "region_graph = FullyFactorized(num_vars=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T13:01:32.902658Z",
     "start_time": "2024-03-14T13:01:32.896040Z"
    }
   },
   "outputs": [],
   "source": [
    "region_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T13:05:52.630911Z",
     "start_time": "2024-03-14T13:05:52.622639Z"
    }
   },
   "outputs": [],
   "source": [
    "region_graph._nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Others available region graphs are _Poon Domingos_ and _QuadTree_, whose imports are showed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T14:36:18.569812Z",
     "start_time": "2024-03-14T14:36:18.526134Z"
    }
   },
   "outputs": [],
   "source": [
    "from cirkit.region_graph.poon_domingos import PoonDomingos\n",
    "from cirkit.region_graph.random_binary_tree import RandomBinaryTree\n",
    "from cirkit.region_graph.fully_factorized import FullyFactorized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to choose both the input and inner layers of our circuit. As input layer we select the _CategoricalLayer_ with 256 categories (the number of pixel values). For the inner layer instead, we choose the _uncollapsed CP_ layer with rank 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T14:36:22.044953Z",
     "start_time": "2024-03-14T14:36:21.997532Z"
    }
   },
   "outputs": [],
   "source": [
    "from cirkit.layers.input.exp_family import CategoricalLayer\n",
    "from cirkit.layers.sum_product import CPLayer\n",
    "from cirkit.layers.input.rbf_kernel import RBFKernelLayer\n",
    "\n",
    "efamily_cls = RBFKernelLayer\n",
    "efamily_kwargs = {}\n",
    "layer_cls = CPLayer\n",
    "layer_kwargs = {'rank': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the tensorized PC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now build our tensorized PC by specifying the region graph and layers we chose previously. In addition, we can scale the architecture by increasing the number of input and inner units. We can also have circuits with multiple output units by choosing _num_classes > 1_. However, in this notebook we only estimate the distribution of the images and marginalize out the class variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure weights are non-negative we reparametrize them via exponentiation. Several reparametrization functions are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T14:36:23.646351Z",
     "start_time": "2024-03-14T14:36:23.593945Z"
    }
   },
   "outputs": [],
   "source": [
    "from cirkit.reparams.leaf import ReparamExp, ReparamLogSoftmax, ReparamSoftmax\n",
    "from cirkit.models.tensorized_circuit import TensorizedPC\n",
    "pc = TensorizedPC.from_region_graph(\n",
    "    region_graph,\n",
    "    num_inner_units=1500,\n",
    "    num_input_units=1500,\n",
    "    efamily_cls=efamily_cls,\n",
    "    efamily_kwargs=efamily_kwargs,\n",
    "    layer_cls=layer_cls,\n",
    "    layer_kwargs=layer_kwargs,\n",
    "    num_classes=1,\n",
    "    reparam=ReparamSoftmax # ReparamLogSoftmax # ReparamExp\n",
    ")\n",
    "pc.to(device)\n",
    "print(pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be79dbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in pc.parameters(): \n",
    "    print (param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb252ed6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1e8199",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cirkit.models.rbf_kernel import RBFCircuitKernel\n",
    "\n",
    "circuit_kernel = RBFCircuitKernel(pc, batch_shape=torch.Size([]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98841a7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b400ac41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cirkit.models.gp import CircuitGP, initial_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b879cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "from uci_datasets import Dataset\n",
    "\n",
    "from ignite.engine import Events, Engine\n",
    "from ignite.metrics import Average, Loss\n",
    "from ignite.contrib.handlers import ProgressBar\n",
    "\n",
    "import gpytorch\n",
    "from gpytorch.mlls import VariationalELBO\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaefe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset(\"gas\")\n",
    "x_train, y_train, x_test, y_test = data.get_split(split=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8905bdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d1accf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_real = x_train[:2208] #32000 # 2053   36584    36584     39063   13281    2672   # RE-RUN # 13279   # 1279   4701  824\n",
    "y_train_real = y_train[:2208]\n",
    "y_train_real = y_train_real.squeeze()\n",
    "x_val = x_train[2208:]\n",
    "y_val = y_train[2208:]\n",
    "y_val = y_val.squeeze()\n",
    "y_test = y_test.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cc07c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a58de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class IdentityMapping(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IdentityMapping, self).__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74111ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f58cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_real.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d527e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(24)\n",
    "torch.manual_seed(24) ####################### CHANGE\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# X_train, y_train = make_data(n_samples)\n",
    "# X_test, y_test = X_train, y_train\n",
    "\n",
    "# x_train, y_train, x_test, y_test\n",
    "\n",
    "ds_train = torch.utils.data.TensorDataset(torch.from_numpy(x_train_real).float(), torch.from_numpy(y_train_real).float())\n",
    "dl_train = torch.utils.data.DataLoader(ds_train, batch_size=batch_size, shuffle=True, drop_last=True) # suffle \n",
    "\n",
    "ds_val = torch.utils.data.TensorDataset(torch.from_numpy(x_val).float(), torch.from_numpy(y_val).float())\n",
    "dl_val = torch.utils.data.DataLoader(ds_val, batch_size=512, shuffle=False)\n",
    "\n",
    "ds_test = torch.utils.data.TensorDataset(torch.from_numpy(x_test).float(), torch.from_numpy(y_test).float())\n",
    "dl_test = torch.utils.data.DataLoader(ds_test, batch_size=512, shuffle=False)\n",
    "\n",
    "# steps = 5e3\n",
    "epochs = 50\n",
    "print(f\"Training with {len(x_train_real)} datapoints for {epochs} epochs\")\n",
    "\n",
    "# Change this boolean to False for SNGP\n",
    "DUE = True\n",
    "\n",
    "input_dim = 128 # input di  # 128\n",
    "# features = 1024 # hidden    128\n",
    "# depth = 2   # 4  6\n",
    "num_outputs = 1 # regression with 1D output\n",
    "# spectral_normalization = True\n",
    "# coeff = 0.95\n",
    "# n_power_iterations = 1\n",
    "# dropout_rate = 0.01\n",
    "\n",
    "# feature_extractor = FCResNet(\n",
    "#     input_dim=input_dim, \n",
    "#     features=features, \n",
    "#     depth=depth, \n",
    "#     spectral_normalization=spectral_normalization, \n",
    "#     coeff=coeff, \n",
    "#     n_power_iterations=n_power_iterations,\n",
    "#     dropout_rate=dropout_rate\n",
    "# )\n",
    "\n",
    "feature_extractor = IdentityMapping()\n",
    "\n",
    "if DUE:\n",
    "    n_inducing_points = 10\n",
    "    kernel = \"HBF\" ################# change \n",
    "    \n",
    "    initial_inducing_points, initial_lengthscale = initial_values(\n",
    "            ds_train, feature_extractor, n_inducing_points\n",
    "    )\n",
    "\n",
    "    gp_model = CircuitGP(\n",
    "        num_outputs=num_outputs,\n",
    "        num_features=input_dim,          # CHANGE features / input_dim\n",
    "        initial_lengthscale=initial_lengthscale,\n",
    "        initial_inducing_points=initial_inducing_points,\n",
    "        circuit=pc\n",
    "        # kernel=kernel,\n",
    "    )\n",
    "\n",
    "    # model = DKL(feature_extractor, gp)\n",
    "\n",
    "    likelihood = GaussianLikelihood()\n",
    "    elbo_fn = VariationalELBO(likelihood, gp_model, num_data=len(ds_train))\n",
    "    loss_fn = lambda x, y: -elbo_fn(x, y)\n",
    "    \n",
    "    # mse_loss_fn = F.mse_loss\n",
    "# else:\n",
    "    # Nothing \n",
    "#     num_gp_features = 128\n",
    "#     num_random_features = 1024\n",
    "#     normalize_gp_features = True\n",
    "#     feature_scale = 2\n",
    "#     ridge_penalty = 1\n",
    "    \n",
    "#     model = Laplace(feature_extractor,\n",
    "#                     features,\n",
    "#                     num_gp_features,\n",
    "#                     normalize_gp_features,\n",
    "#                     num_random_features,\n",
    "#                     num_outputs,\n",
    "#                     len(ds_train),\n",
    "#                     batch_size,\n",
    "#                     ridge_penalty=ridge_penalty,\n",
    "#                     feature_scale=feature_scale\n",
    "#                    )\n",
    "\n",
    "#     loss_fn = F.mse_loss # MSE\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gp_model = gp_model.cuda()\n",
    "    if DUE:\n",
    "        likelihood = likelihood.cuda()\n",
    "\n",
    "# learning rate   \n",
    "lr = 1e-3\n",
    "\n",
    "parameters = [\n",
    "    {\"params\": gp_model.parameters(), \"lr\": lr},\n",
    "]\n",
    "\n",
    "if DUE:\n",
    "    parameters.append({\"params\": likelihood.parameters(), \"lr\": lr})\n",
    "    \n",
    "    \n",
    "optimizer = torch.optim.Adam(parameters)\n",
    "pbar = ProgressBar()\n",
    "\n",
    "step_counter = 0\n",
    "\n",
    "def step(engine, batch):\n",
    "    \n",
    "    global step_counter\n",
    "    step_counter += 1\n",
    "    \n",
    "    gp_model.train()\n",
    "    if DUE:\n",
    "        likelihood.train()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    x, y = batch\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "\n",
    "    y_pred = gp_model(x) # get y\n",
    "    \n",
    "    if not DUE:\n",
    "        y_pred.squeeze_()\n",
    "    \n",
    "#     print(\"y_pred\", y_pred)\n",
    "#     print(\"y_pred_real\", likelihood(y_pred).mean.cpu())\n",
    "#     print(\"y\", y)\n",
    "    loss = loss_fn(y_pred, y) # loss\n",
    "    # print(\"loss\", loss)\n",
    "    \n",
    "    if torch.isnan(loss).any():\n",
    "        print(f\"Step {step_counter}: NaN detected in loss.\")\n",
    "        print(\"loss\", loss)\n",
    "        print(\"y_pred\", y_pred)\n",
    "    \n",
    "    if torch.isnan(loss).any():\n",
    "        print(\"NaN detected in loss, saving model and stopping.\")\n",
    "        # Save model weights before termination\n",
    "        torch.save(gp_model.state_dict(), 'model_weights_before_nan.pt')\n",
    "        engine.terminate()\n",
    "        return\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def eval_step(engine, batch):\n",
    "    gp_model.eval() # set to eval\n",
    "    if DUE:\n",
    "        likelihood.eval()\n",
    "    \n",
    "    x, y = batch\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "\n",
    "    y_pred = gp_model(x)\n",
    "    \n",
    "    # eval_mes_loss = mse_loss_fn(y_pred, y) # MSE eval\n",
    "            \n",
    "    return y_pred, y\n",
    "\n",
    "    \n",
    "trainer = Engine(step)\n",
    "evaluator = Engine(eval_step)\n",
    "\n",
    "metric = Average()\n",
    "metric.attach(trainer, \"loss\")\n",
    "pbar.attach(trainer)\n",
    "\n",
    "if DUE:\n",
    "    metric = Loss(lambda y_pred, y: - likelihood.expected_log_prob(y, y_pred).mean())\n",
    "    # metric = Loss(lambda y_pred, y: F.mse_loss(likelihood(y_pred).mean.cpu(), y))\n",
    "else:\n",
    "    metric = Loss(lambda y_pred, y: F.mse_loss(y_pred[0].squeeze(), y))\n",
    "\n",
    "\n",
    "metric.attach(evaluator, \"loss\")\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED(every=int(epochs/20) + 1))\n",
    "def log_results(trainer):\n",
    "    evaluator.run(dl_val) # val dataset\n",
    "    print(f\"Results - Epoch: {trainer.state.epoch} - \"\n",
    "          f\"Val Loss: {evaluator.state.metrics['loss']:.2f} - \"\n",
    "          f\"Train Loss: {trainer.state.metrics['loss']:.2f}\")\n",
    "\n",
    "    \n",
    "if not DUE:\n",
    "    @trainer.on(Events.EPOCH_STARTED)\n",
    "    def reset_precision_matrix(trainer):\n",
    "        gp_model.reset_precision_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cab3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, param in enumerate(gp_model.parameters()): \n",
    "    # if (index==2):\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82340f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.run(dl_train, max_epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537363e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in likelihood.parameters(): \n",
    "    print (param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c666fe1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_model.eval()\n",
    "if DUE:\n",
    "    likelihood.eval()\n",
    "\n",
    "all_mse = []\n",
    "    \n",
    "with torch.no_grad(), gpytorch.settings.num_likelihood_samples(100):\n",
    "    \n",
    "    xx_split = np.array_split(x_test, 40)       ############# CHANGE\n",
    "    yy_split = np.array_split(y_test, 40)\n",
    "    \n",
    "    for index in range(len(xx_split)):\n",
    "    \n",
    "        xx = torch.from_numpy(xx_split[index]).float()\n",
    "        yy = torch.from_numpy(yy_split[index]).float()\n",
    "        pred_test = gp_model(xx)\n",
    "        ol = likelihood(pred_test)\n",
    "        output = ol.mean.cpu()\n",
    "        mse = F.mse_loss(output, yy)\n",
    "        all_mse.append(mse)\n",
    "    \n",
    "    \n",
    "average_mse = sum(all_mse) / len(all_mse)\n",
    "average_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2b5f09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab37ce73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53c2c82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63fb408",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5011843",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed631b25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d906bdc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c13f66c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d1c1b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8794c43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed311e9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0798ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T14:36:25.724719Z",
     "start_time": "2024-03-14T14:36:25.680948Z"
    }
   },
   "outputs": [],
   "source": [
    "pc.input_layer.params.param.shape\n",
    "# (self.num_vars, self.num_output_units, self.num_replicas, self.num_suff_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T14:35:35.437628Z",
     "start_time": "2024-03-14T14:35:35.393853Z"
    }
   },
   "outputs": [],
   "source": [
    "pc.scope_layer.scope.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T14:36:27.302533Z",
     "start_time": "2024-03-14T14:36:27.260539Z"
    }
   },
   "outputs": [],
   "source": [
    "pc.inner_layers[0].params_in() #.param #.shape #.param.shape\n",
    "# (F, H, I, O)\n",
    "# (fold count, arity, input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T13:11:52.080085Z",
     "start_time": "2024-03-14T13:11:52.073368Z"
    }
   },
   "outputs": [],
   "source": [
    "from cirkit.models.rbf_kernel import RBFCircuitKernel\n",
    "\n",
    "circuit_kernel = RBFCircuitKernel(pc, batch_shape=torch.Size([]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T13:06:06.337943Z",
     "start_time": "2024-03-14T13:06:06.330408Z"
    }
   },
   "outputs": [],
   "source": [
    "circuit_kernel(x1.squeeze(), x2.squeeze()).evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T13:06:07.504758Z",
     "start_time": "2024-03-14T13:06:07.498492Z"
    }
   },
   "outputs": [],
   "source": [
    "x1.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T13:06:09.102855Z",
     "start_time": "2024-03-14T13:06:09.096848Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57dea52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4666592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "\n",
    "pc.input_layer.params.param = torch.nn.Parameter(torch.log(torch.ones(tuple(pc.input_layer.params.shape))*3.3))\n",
    "# pc.inner_layers[0].params_in.param = torch.nn.Parameter(torch.log(0.25*torch.ones(tuple(pc.inner_layers[0].params_in.shape))))\n",
    "# pc.inner_layers[0].params_in = torch.nn.Parameter(torch.ones(tuple(pc.inner_layers[0].params_in.shape))*3.3)\n",
    "# pc.inner_layers[1].params_in = torch.nn.Parameter(torch.ones(tuple(pc.inner_layers[1].params_in.shape))*3.3)\n",
    "# pc.inner_layers[2].params_in = torch.nn.Parameter(torch.ones(tuple(pc.inner_layers[2].params_in.shape))*3.3)\n",
    "# pc.inner_layers[3].params_in = torch.nn.Parameter(torch.ones(tuple(pc.inner_layers[3].params_in.shape))*3.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a99ab32",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.inner_layers[0].params_in() #.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de51e1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.randn(3, 8, 1)\n",
    "x2 = torch.randn(3, 8, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e7872d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc(x1, x2).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_pc(x1, x2): \n",
    "    return pc(x1.unsqueeze(-1), x2.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "eval_pc(x1.squeeze(), x2.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade0b238",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.kernels import RBFKernel\n",
    "\n",
    "# x = torch.randn(3, 5)\n",
    "covar_module = RBFKernel()\n",
    "covar_module.lengthscale = torch.tensor(3.3)\n",
    "covar_module(x1.squeeze(), x2.squeeze()).evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d4a047",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c4d3c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.kernels import RBFKernel\n",
    "x = torch.randn(3, 2)\n",
    "RBFKernel().lengthscale = torch.tensor(3.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487c6d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test RBF input output = RBF kernel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161a48ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.kernels import RBFKernel, SpectralMixtureKernel\n",
    "\n",
    "x = torch.randn(3, 5)\n",
    "covar_module = SpectralMixtureKernel(num_mixtures=2, ard_num_dims=5)\n",
    "covar_module.mixture_scales = torch.tensor(3.3).expand(1, 2, 1, 5)\n",
    "covar_module.mixture_means = torch.tensor(2.2).expand(1, 2, 1, 5)\n",
    "covar_module.mixture_weights = torch.tensor([0.5]).expand(1, 2, 1, 5)\n",
    "covar_module(x).evaluate()\n",
    "# covar_module.lengthscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3a04d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cirkit.layers.input.sm_kernel import SMKernelLayer\n",
    "input_la = SMKernelLayer(num_vars=5, num_output_units=1)\n",
    "\n",
    "input_la.params = torch.nn.Parameter(torch.ones((5,1))*3.3)\n",
    "\n",
    "# input_la(x1, x2).squeeze().shape\n",
    "\n",
    "# input_la(x.unsqueeze(-1), x.unsqueeze(-1)).shape\n",
    "\n",
    "torch.prod(torch.exp(input_la(x.unsqueeze(-1), x.unsqueeze(-1)).squeeze()), dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f025e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_la = RBFKernelLayer(num_vars=20, num_output_units=1)\n",
    "\n",
    "input_la.params = torch.nn.Parameter(torch.ones((20,1))*3.3)\n",
    "\n",
    "# input_la(x1, x2).squeeze().shape\n",
    "torch.prod(input_la(x1, x1).squeeze(), dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6facf1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.kernels import RBFKernel\n",
    "\n",
    "x = torch.randn(3, 5)\n",
    "covar_module = RBFKernel()\n",
    "covar_module.lengthscale = torch.tensor(3.3)\n",
    "covar_module(x).evaluate()\n",
    "# covar_module.lengthscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0dc104",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cirkit.layers.input.rbf_kernel import RBFKernelLayer\n",
    "input_la = RBFKernelLayer(num_vars=5, num_output_units=1)\n",
    "\n",
    "input_la.params.param = torch.nn.Parameter(torch.log(torch.ones(tuple(input_la.params.shape))*3.3))\n",
    "# pc.input_layer.params.param = torch.nn.Parameter(torch.log(torch.ones(tuple(pc.input_layer.params.shape))*3.3))\n",
    "\n",
    "# input_la(x1, x2).squeeze().shape\n",
    "\n",
    "# input_la(x.unsqueeze(-1), x.unsqueeze(-1)).shape\n",
    "\n",
    "torch.prod(torch.exp(input_la(x.unsqueeze(-1), x.unsqueeze(-1)).squeeze()), dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bd562b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd606c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf6dd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.linspace(0, 1, 3)\n",
    "torch.sin(train_x * (2 * math.pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a2c753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "# train_x = torch.linspace(0, 1, 3)\n",
    "# train_y = torch.sin(train_x * (2 * math.pi))\n",
    "train_x = torch.rand((3, 5))\n",
    "train_y = torch.rand((3))\n",
    "\n",
    "covar_module = SpectralMixtureKernel(num_mixtures=4, ard_num_dims=5)\n",
    "covar_module.initialize_from_data(train_x, train_y)\n",
    "covar_module(train_x).evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f94551",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# SM kernel check\n",
    "\n",
    "from cirkit.layers.input.sm_kernel import SMKernelLayer\n",
    "input_la = SMKernelLayer(num_vars=5, num_output_units=4)\n",
    "\n",
    "input_la.params_mu.param = torch.nn.Parameter(covar_module.mixture_means)\n",
    "input_la.params_sigma.param = torch.nn.Parameter(torch.log(covar_module.mixture_scales))\n",
    "\n",
    "\n",
    "to_be_weighted = input_la(train_x.unsqueeze(-1), train_x.unsqueeze(-1))\n",
    "\n",
    "# to_be_weighted = torch.prod(to_be_weighted, dim=2, keepdim=True) / 5\n",
    "\n",
    "tensor1_expanded = covar_module.mixture_weights.expand_as(to_be_weighted.squeeze(-1))\n",
    "\n",
    "# Element-wise multiplication and then sum over the inner product dimension (dimension 3 after squeeze)\n",
    "finalfinal = (tensor1_expanded * to_be_weighted.squeeze(-1)).sum(dim=3).squeeze()\n",
    "\n",
    "torch.prod(finalfinal, dim=-1, keepdim=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0e3da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.Parameter(covar_module.mixture_means).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be1f889",
   "metadata": {},
   "outputs": [],
   "source": [
    "covar_module.mixture_scales.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b267ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "covar_module(x1).evaluate().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d377b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.unsqueeze(-1).unsqueeze(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c538f29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = torch.tensor([[-0.6281], [ 0.1011], [ 0.0664]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbb7331",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cirkit.layers.input.rbf_kernel import RBFKernelLayer\n",
    "input_la = RBFKernelLayer(num_vars=2, num_output_units=1)\n",
    "\n",
    "input_la.params = torch.nn.Parameter(torch.ones((1,1))*3.3)\n",
    "\n",
    "input_la(x_2.unsqueeze(-1), x_2.unsqueeze(-1)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45183425",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_la.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a28677",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.ones((2,1))*3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4850262b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.unsqueeze(-1).unsqueeze(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed03208",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_la(x.unsqueeze(-1), x.unsqueeze(-1)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41b397a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2.unsqueeze(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a6de01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756535e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cdist(x1, x2, p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(data_train, shuffle=True, batch_size=256)\n",
    "test_dataloader = DataLoader(data_test, shuffle=False, batch_size=256)\n",
    "optimizer = optim.SGD(pc.parameters(), lr=0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the constructed PC is not necessarily normalized, we construct the integral circuit that will compute the partition function. Note that parameters are shared and therefore there is no additional memory required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cirkit.models.functional import integrate\n",
    "pc_pf = integrate(pc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we optimize the parameters for 5 epochs by minimizing the negative log-likelohood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "for epoch_idx in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for batch, _ in train_dataloader:\n",
    "        batch = batch.to(device).unsqueeze(dim=-1)  # Add a channel dimension\n",
    "        log_score = pc(batch)\n",
    "        log_pf = pc_pf(batch)     # Compute the partition function\n",
    "        lls = log_score - log_pf  # Compute the log-likelihood\n",
    "        loss = -torch.mean(lls)   # The loss is the negative average log-likelihood\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        running_loss += loss * len(batch)\n",
    "        # Clamp the parameters to ensure they are in the intended domain\n",
    "        # This is needed if we do not use any reparametrization to ensure parameters non-negativity\n",
    "        # In our case, clamping is disable becuase we reparameterize via exponentiation (see above)\n",
    "        #for layer in model.inner_layers:\n",
    "        #    layer.clamp_params()\n",
    "    print(f\"Epoch {epoch_idx}: Average NLL: {running_loss / len(data_train):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then evaluate our model on test data by computing the average log-likelihood and bits per dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    pc.eval()\n",
    "    log_pf = pc_pf(torch.empty((), device=device))  # Compute the partition function once for testing\n",
    "    test_lls = 0.0\n",
    "    for batch, _ in test_dataloader:\n",
    "        log_score = pc(batch.to(device).unsqueeze(dim=-1))\n",
    "        lls = log_score - log_pf\n",
    "        test_lls += lls.sum().item()\n",
    "    average_ll = test_lls / len(data_test)\n",
    "    bpd = -average_ll / (num_variables * np.log(2.0))\n",
    "    print(f\"Average test LL: {average_ll:.3f}\")\n",
    "    print(f\"Bits per dimension: {bpd}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4acbcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760a9727",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf92f8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import gpytorch\n",
    "\n",
    "# from ..functions import RBFCovariance\n",
    "# from ..settings import trace_mode\n",
    "from gpytorch.kernels import Kernel\n",
    "\n",
    "\n",
    "def postprocess_rbf(dist_mat):\n",
    "    return dist_mat.div_(-2).exp_()\n",
    "\n",
    "\n",
    "class TestRBFKernel(Kernel):\n",
    "    r\"\"\"\n",
    "    Computes a covariance matrix based on the RBF (squared exponential) kernel\n",
    "    between inputs :math:`\\mathbf{x_1}` and :math:`\\mathbf{x_2}`:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "       \\begin{equation*}\n",
    "          k_{\\text{RBF}}(\\mathbf{x_1}, \\mathbf{x_2}) = \\exp \\left( -\\frac{1}{2}\n",
    "          (\\mathbf{x_1} - \\mathbf{x_2})^\\top \\Theta^{-2} (\\mathbf{x_1} - \\mathbf{x_2}) \\right)\n",
    "       \\end{equation*}\n",
    "\n",
    "    where :math:`\\Theta` is a :attr:`lengthscale` parameter.\n",
    "    See :class:`gpytorch.kernels.Kernel` for descriptions of the lengthscale options.\n",
    "\n",
    "    .. note::\n",
    "\n",
    "        This kernel does not have an `outputscale` parameter. To add a scaling parameter,\n",
    "        decorate this kernel with a :class:`gpytorch.kernels.ScaleKernel`.\n",
    "\n",
    "    Args:\n",
    "        :attr:`ard_num_dims` (int, optional):\n",
    "            Set this if you want a separate lengthscale for each\n",
    "            input dimension. It should be `d` if :attr:`x1` is a `n x d` matrix. Default: `None`\n",
    "        :attr:`batch_shape` (torch.Size, optional):\n",
    "            Set this if you want a separate lengthscale for each\n",
    "            batch of input data. It should be `b` if :attr:`x1` is a `b x n x d` tensor. Default: `torch.Size([])`.\n",
    "        :attr:`active_dims` (tuple of ints, optional):\n",
    "            Set this if you want to compute the covariance of only a few input dimensions. The ints\n",
    "            corresponds to the indices of the dimensions. Default: `None`.\n",
    "        :attr:`lengthscale_prior` (Prior, optional):\n",
    "            Set this if you want to apply a prior to the lengthscale parameter.  Default: `None`.\n",
    "        :attr:`lengthscale_constraint` (Constraint, optional):\n",
    "            Set this if you want to apply a constraint to the lengthscale parameter. Default: `Positive`.\n",
    "        :attr:`eps` (float):\n",
    "            The minimum value that the lengthscale can take (prevents divide by zero errors). Default: `1e-6`.\n",
    "\n",
    "    Attributes:\n",
    "        :attr:`lengthscale` (Tensor):\n",
    "            The lengthscale parameter. Size/shape of parameter depends on the\n",
    "            :attr:`ard_num_dims` and :attr:`batch_shape` arguments.\n",
    "\n",
    "    Example:\n",
    "        >>> x = torch.randn(10, 5)\n",
    "        >>> # Non-batch: Simple option\n",
    "        >>> covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "        >>> # Non-batch: ARD (different lengthscale for each input dimension)\n",
    "        >>> covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=5))\n",
    "        >>> covar = covar_module(x)  # Output: LazyTensor of size (10 x 10)\n",
    "        >>>\n",
    "        >>> batch_x = torch.randn(2, 10, 5)\n",
    "        >>> # Batch: Simple option\n",
    "        >>> covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "        >>> # Batch: different lengthscale for each batch\n",
    "        >>> covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(batch_shape=torch.Size([2])))\n",
    "        >>> covar = covar_module(x)  # Output: LazyTensor of size (2 x 10 x 10)\n",
    "    \"\"\"\n",
    "\n",
    "    has_lengthscale = True\n",
    "\n",
    "    def forward(self, x1, x2, diag=False, **params):\n",
    "\n",
    "        x1_ = x1.div(self.lengthscale)\n",
    "        x2_ = x2.div(self.lengthscale)\n",
    "        \n",
    "        # print (\"x1, x2\", x1_, x2_)\n",
    "        \n",
    "        return self.covar_dist(\n",
    "            x1_, x2_, square_dist=True, diag=diag, dist_postprocess_func=postprocess_rbf, postprocess=True, **params\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e9a322",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_kernel = TestRBFKernel()\n",
    "test_kernel.lengthscale = torch.tensor(3.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0114f19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_kernel.lengthscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4731298",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_kernel(x1.squeeze(),x2.squeeze()).evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8887c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d2c61d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef03dad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5028f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cirkit1",
   "language": "python",
   "name": "cirkit1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
