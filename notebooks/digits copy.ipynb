{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and evaluate a PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T14:36:00.305482Z",
     "start_time": "2024-03-14T14:35:59.231534Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T14:36:01.996183Z",
     "start_time": "2024-03-14T14:36:01.950399Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")  # The device to use, e.g., \"cpu\", \"cuda\", \"cuda:1\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T14:36:03.245505Z",
     "start_time": "2024-03-14T14:36:03.190582Z"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the random seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T14:36:05.899377Z",
     "start_time": "2024-03-14T14:36:05.858609Z"
    }
   },
   "outputs": [],
   "source": [
    "random.seed(4)\n",
    "np.random.seed(4)\n",
    "torch.manual_seed(4)\n",
    "# if 'cuda' in device.type:\n",
    "#     torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the training and test splits of MNIST, and preprocess them by flattening the tensor images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T14:36:10.329915Z",
     "start_time": "2024-03-14T14:36:09.956962Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: (255 * x.view(-1)).long())\n",
    "])\n",
    "data_train = datasets.MNIST('datasets', train=True, download=True, transform=transform)\n",
    "data_test = datasets.MNIST('datasets', train=False, download=True, transform=transform)\n",
    "num_variables = data_train[0][0].shape[0]\n",
    "height, width = 28, 28\n",
    "print(f\"Number of variables: {num_variables}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T12:41:27.661486Z",
     "start_time": "2024-03-14T12:41:27.534444Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.matshow(data_train[0][0].reshape(28, 28), cmap='gray')\n",
    "plt.title(f\"Class: {data_train[0][1]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating the region graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a _Quad Graph_ region graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T14:36:19.883137Z",
     "start_time": "2024-03-14T14:36:19.844640Z"
    }
   },
   "outputs": [],
   "source": [
    "from cirkit.region_graph.quad_tree import QuadTree\n",
    "# region_graph = QuadTree(width, height, struct_decomp=False)\n",
    "# region_graph = RandomBinaryTree(num_vars=11, depth=2, num_repetitions=1)\n",
    "region_graph = FullyFactorized(num_vars=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T13:01:32.902658Z",
     "start_time": "2024-03-14T13:01:32.896040Z"
    }
   },
   "outputs": [],
   "source": [
    "region_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T13:05:52.630911Z",
     "start_time": "2024-03-14T13:05:52.622639Z"
    }
   },
   "outputs": [],
   "source": [
    "region_graph._nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Others available region graphs are _Poon Domingos_ and _QuadTree_, whose imports are showed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T14:36:18.569812Z",
     "start_time": "2024-03-14T14:36:18.526134Z"
    }
   },
   "outputs": [],
   "source": [
    "from cirkit.region_graph.poon_domingos import PoonDomingos\n",
    "from cirkit.region_graph.random_binary_tree import RandomBinaryTree\n",
    "from cirkit.region_graph.fully_factorized import FullyFactorized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to choose both the input and inner layers of our circuit. As input layer we select the _CategoricalLayer_ with 256 categories (the number of pixel values). For the inner layer instead, we choose the _uncollapsed CP_ layer with rank 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T14:36:22.044953Z",
     "start_time": "2024-03-14T14:36:21.997532Z"
    }
   },
   "outputs": [],
   "source": [
    "from cirkit.layers.input.exp_family import CategoricalLayer\n",
    "from cirkit.layers.sum_product import CPLayer\n",
    "from cirkit.layers.input.rbf_kernel import RBFKernelLayer\n",
    "\n",
    "efamily_cls = RBFKernelLayer\n",
    "efamily_kwargs = {}\n",
    "layer_cls = CPLayer\n",
    "layer_kwargs = {'rank': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the tensorized PC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now build our tensorized PC by specifying the region graph and layers we chose previously. In addition, we can scale the architecture by increasing the number of input and inner units. We can also have circuits with multiple output units by choosing _num_classes > 1_. However, in this notebook we only estimate the distribution of the images and marginalize out the class variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure weights are non-negative we reparametrize them via exponentiation. Several reparametrization functions are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T14:36:23.646351Z",
     "start_time": "2024-03-14T14:36:23.593945Z"
    }
   },
   "outputs": [],
   "source": [
    "from cirkit.reparams.leaf import ReparamExp, ReparamLogSoftmax, ReparamSoftmax\n",
    "from cirkit.models.tensorized_circuit import TensorizedPC\n",
    "pc = TensorizedPC.from_region_graph(\n",
    "    region_graph,\n",
    "    num_inner_units=50,\n",
    "    num_input_units=50,\n",
    "    efamily_cls=efamily_cls,\n",
    "    efamily_kwargs=efamily_kwargs,\n",
    "    layer_cls=layer_cls,\n",
    "    layer_kwargs=layer_kwargs,\n",
    "    num_classes=1,\n",
    "    reparam=ReparamSoftmax # ReparamLogSoftmax # ReparamExp\n",
    ")\n",
    "pc.to(device)\n",
    "print(pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be79dbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in pc.parameters(): \n",
    "    print (param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb252ed6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1e8199",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cirkit.models.rbf_kernel import RBFCircuitKernel\n",
    "\n",
    "circuit_kernel = RBFCircuitKernel(pc, batch_shape=torch.Size([]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b400ac41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cirkit.models.gp import CircuitGP, initial_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b879cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "from uci_datasets import Dataset\n",
    "\n",
    "from ignite.engine import Events, Engine\n",
    "from ignite.metrics import Average, Loss\n",
    "from ignite.contrib.handlers import ProgressBar\n",
    "\n",
    "import gpytorch\n",
    "from gpytorch.mlls import VariationalELBO\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaefe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset(\"elevators\")\n",
    "x_train, y_train, x_test, y_test = data.get_split(split=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8905bdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d1accf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_real = x_train[:13281] #32000 # 2053   36584    36584     39063   13281    2672   # RE-RUN # 13279   # 1279   4701  824\n",
    "y_train_real = y_train[:13281]\n",
    "y_train_real = y_train_real.squeeze()\n",
    "x_val = x_train[13281:]\n",
    "y_val = y_train[13281:]\n",
    "y_val = y_val.squeeze()\n",
    "y_test = y_test.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cc07c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a58de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class IdentityMapping(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IdentityMapping, self).__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74111ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f58cb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d527e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(24)\n",
    "torch.manual_seed(24) ####################### CHANGE\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "# X_train, y_train = make_data(n_samples)\n",
    "# X_test, y_test = X_train, y_train\n",
    "\n",
    "# x_train, y_train, x_test, y_test\n",
    "\n",
    "ds_train = torch.utils.data.TensorDataset(torch.from_numpy(x_train_real).float(), torch.from_numpy(y_train_real).float())\n",
    "dl_train = torch.utils.data.DataLoader(ds_train, batch_size=batch_size, shuffle=True, drop_last=True) # suffle \n",
    "\n",
    "ds_val = torch.utils.data.TensorDataset(torch.from_numpy(x_val).float(), torch.from_numpy(y_val).float())\n",
    "dl_val = torch.utils.data.DataLoader(ds_val, batch_size=512, shuffle=False)\n",
    "\n",
    "ds_test = torch.utils.data.TensorDataset(torch.from_numpy(x_test).float(), torch.from_numpy(y_test).float())\n",
    "dl_test = torch.utils.data.DataLoader(ds_test, batch_size=512, shuffle=False)\n",
    "\n",
    "# steps = 5e3\n",
    "epochs = 50\n",
    "print(f\"Training with {len(x_train_real)} datapoints for {epochs} epochs\")\n",
    "\n",
    "# Change this boolean to False for SNGP\n",
    "DUE = True\n",
    "\n",
    "input_dim = 18 # input di  # 128\n",
    "# features = 1024 # hidden    128\n",
    "# depth = 2   # 4  6\n",
    "num_outputs = 1 # regression with 1D output\n",
    "# spectral_normalization = True\n",
    "# coeff = 0.95\n",
    "# n_power_iterations = 1\n",
    "# dropout_rate = 0.01\n",
    "\n",
    "# feature_extractor = FCResNet(\n",
    "#     input_dim=input_dim, \n",
    "#     features=features, \n",
    "#     depth=depth, \n",
    "#     spectral_normalization=spectral_normalization, \n",
    "#     coeff=coeff, \n",
    "#     n_power_iterations=n_power_iterations,\n",
    "#     dropout_rate=dropout_rate\n",
    "# )\n",
    "\n",
    "feature_extractor = IdentityMapping()\n",
    "\n",
    "if DUE:\n",
    "    n_inducing_points = 100\n",
    "    kernel = \"HBF\" ################# change \n",
    "    \n",
    "    initial_inducing_points, initial_lengthscale = initial_values(\n",
    "            ds_train, feature_extractor, n_inducing_points\n",
    "    )\n",
    "\n",
    "    gp_model = CircuitGP(\n",
    "        num_outputs=num_outputs,\n",
    "        num_features=input_dim,          # CHANGE features / input_dim\n",
    "        initial_lengthscale=initial_lengthscale,\n",
    "        initial_inducing_points=initial_inducing_points,\n",
    "        circuit=pc\n",
    "        # kernel=kernel,\n",
    "    )\n",
    "\n",
    "    # model = DKL(feature_extractor, gp)\n",
    "\n",
    "    likelihood = GaussianLikelihood()\n",
    "    elbo_fn = VariationalELBO(likelihood, gp_model, num_data=len(ds_train))\n",
    "    loss_fn = lambda x, y: -elbo_fn(x, y)\n",
    "    \n",
    "    # mse_loss_fn = F.mse_loss\n",
    "# else:\n",
    "    # Nothing \n",
    "#     num_gp_features = 128\n",
    "#     num_random_features = 1024\n",
    "#     normalize_gp_features = True\n",
    "#     feature_scale = 2\n",
    "#     ridge_penalty = 1\n",
    "    \n",
    "#     model = Laplace(feature_extractor,\n",
    "#                     features,\n",
    "#                     num_gp_features,\n",
    "#                     normalize_gp_features,\n",
    "#                     num_random_features,\n",
    "#                     num_outputs,\n",
    "#                     len(ds_train),\n",
    "#                     batch_size,\n",
    "#                     ridge_penalty=ridge_penalty,\n",
    "#                     feature_scale=feature_scale\n",
    "#                    )\n",
    "\n",
    "#     loss_fn = F.mse_loss # MSE\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gp_model = gp_model.cuda()\n",
    "    if DUE:\n",
    "        likelihood = likelihood.cuda()\n",
    "\n",
    "# learning rate   \n",
    "lr = 1e-3\n",
    "\n",
    "parameters = [\n",
    "    {\"params\": gp_model.parameters(), \"lr\": lr},\n",
    "]\n",
    "\n",
    "if DUE:\n",
    "    parameters.append({\"params\": likelihood.parameters(), \"lr\": lr})\n",
    "    \n",
    "    \n",
    "optimizer = torch.optim.Adam(parameters)\n",
    "pbar = ProgressBar()\n",
    "\n",
    "def step(engine, batch):\n",
    "    gp_model.train()\n",
    "    if DUE:\n",
    "        likelihood.train()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    x, y = batch\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "\n",
    "    y_pred = gp_model(x) # get y\n",
    "    \n",
    "    if not DUE:\n",
    "        y_pred.squeeze_()\n",
    "    \n",
    "#     print(\"y_pred\", y_pred)\n",
    "#     print(\"y_pred_real\", likelihood(y_pred).mean.cpu())\n",
    "#     print(\"y\", y)\n",
    "    loss = loss_fn(y_pred, y) # loss\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def eval_step(engine, batch):\n",
    "    gp_model.eval() # set to eval\n",
    "    if DUE:\n",
    "        likelihood.eval()\n",
    "    \n",
    "    x, y = batch\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "\n",
    "    y_pred = gp_model(x)\n",
    "    \n",
    "    # eval_mes_loss = mse_loss_fn(y_pred, y) # MSE eval\n",
    "            \n",
    "    return y_pred, y\n",
    "\n",
    "    \n",
    "trainer = Engine(step)\n",
    "evaluator = Engine(eval_step)\n",
    "\n",
    "metric = Average()\n",
    "metric.attach(trainer, \"loss\")\n",
    "pbar.attach(trainer)\n",
    "\n",
    "if DUE:\n",
    "    metric = Loss(lambda y_pred, y: - likelihood.expected_log_prob(y, y_pred).mean())\n",
    "    # metric = Loss(lambda y_pred, y: F.mse_loss(likelihood(y_pred).mean.cpu(), y))\n",
    "else:\n",
    "    metric = Loss(lambda y_pred, y: F.mse_loss(y_pred[0].squeeze(), y))\n",
    "\n",
    "\n",
    "metric.attach(evaluator, \"loss\")\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED(every=int(epochs/20) + 1))\n",
    "def log_results(trainer):\n",
    "    evaluator.run(dl_val) # val dataset\n",
    "    print(f\"Results - Epoch: {trainer.state.epoch} - \"\n",
    "          f\"Val Loss: {evaluator.state.metrics['loss']:.2f} - \"\n",
    "          f\"Train Loss: {trainer.state.metrics['loss']:.2f}\")\n",
    "\n",
    "    \n",
    "if not DUE:\n",
    "    @trainer.on(Events.EPOCH_STARTED)\n",
    "    def reset_precision_matrix(trainer):\n",
    "        gp_model.reset_precision_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cab3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in gp_model.parameters(): \n",
    "    print(param.shape)\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82340f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.run(dl_train, max_epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537363e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c666fe1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_model.eval()\n",
    "if DUE:\n",
    "    likelihood.eval()\n",
    "\n",
    "all_mse = []\n",
    "    \n",
    "with torch.no_grad(), gpytorch.settings.num_likelihood_samples(100):\n",
    "    \n",
    "    xx_split = np.array_split(x_test, 40)       ############# CHANGE\n",
    "    yy_split = np.array_split(y_test, 40)\n",
    "    \n",
    "    for index in range(len(xx_split)):\n",
    "    \n",
    "        xx = torch.from_numpy(xx_split[index]).float()\n",
    "        yy = torch.from_numpy(yy_split[index]).float()\n",
    "        pred_test = gp_model(xx)\n",
    "        ol = likelihood(pred_test)\n",
    "        output = ol.mean.cpu()\n",
    "        mse = F.mse_loss(output, yy)\n",
    "        all_mse.append(mse)\n",
    "    \n",
    "    \n",
    "average_mse = sum(all_mse) / len(all_mse)\n",
    "average_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2b5f09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab37ce73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53c2c82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63fb408",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5011843",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed631b25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d906bdc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c13f66c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d1c1b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8794c43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed311e9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0798ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T14:36:25.724719Z",
     "start_time": "2024-03-14T14:36:25.680948Z"
    }
   },
   "outputs": [],
   "source": [
    "pc.input_layer.params.param.shape\n",
    "# (self.num_vars, self.num_output_units, self.num_replicas, self.num_suff_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T14:35:35.437628Z",
     "start_time": "2024-03-14T14:35:35.393853Z"
    }
   },
   "outputs": [],
   "source": [
    "pc.scope_layer.scope.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T14:36:27.302533Z",
     "start_time": "2024-03-14T14:36:27.260539Z"
    }
   },
   "outputs": [],
   "source": [
    "pc.inner_layers[0].params_in() #.param #.shape #.param.shape\n",
    "# (F, H, I, O)\n",
    "# (fold count, arity, input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T13:11:52.080085Z",
     "start_time": "2024-03-14T13:11:52.073368Z"
    }
   },
   "outputs": [],
   "source": [
    "from cirkit.models.rbf_kernel import RBFCircuitKernel\n",
    "\n",
    "circuit_kernel = RBFCircuitKernel(pc, batch_shape=torch.Size([]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T13:06:06.337943Z",
     "start_time": "2024-03-14T13:06:06.330408Z"
    }
   },
   "outputs": [],
   "source": [
    "circuit_kernel(x1.squeeze(), x2.squeeze()).evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T13:06:07.504758Z",
     "start_time": "2024-03-14T13:06:07.498492Z"
    }
   },
   "outputs": [],
   "source": [
    "x1.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T13:06:09.102855Z",
     "start_time": "2024-03-14T13:06:09.096848Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57dea52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4666592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "\n",
    "pc.input_layer.params.param = torch.nn.Parameter(torch.log(torch.ones(tuple(pc.input_layer.params.shape))*3.3))\n",
    "# pc.inner_layers[0].params_in.param = torch.nn.Parameter(torch.log(0.25*torch.ones(tuple(pc.inner_layers[0].params_in.shape))))\n",
    "# pc.inner_layers[0].params_in = torch.nn.Parameter(torch.ones(tuple(pc.inner_layers[0].params_in.shape))*3.3)\n",
    "# pc.inner_layers[1].params_in = torch.nn.Parameter(torch.ones(tuple(pc.inner_layers[1].params_in.shape))*3.3)\n",
    "# pc.inner_layers[2].params_in = torch.nn.Parameter(torch.ones(tuple(pc.inner_layers[2].params_in.shape))*3.3)\n",
    "# pc.inner_layers[3].params_in = torch.nn.Parameter(torch.ones(tuple(pc.inner_layers[3].params_in.shape))*3.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a99ab32",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.inner_layers[0].params_in() #.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de51e1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.randn(3, 8, 1)\n",
    "x2 = torch.randn(3, 8, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e7872d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc(x1, x2).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_pc(x1, x2): \n",
    "    return pc(x1.unsqueeze(-1), x2.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "eval_pc(x1.squeeze(), x2.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade0b238",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.kernels import RBFKernel\n",
    "\n",
    "# x = torch.randn(3, 5)\n",
    "covar_module = RBFKernel()\n",
    "covar_module.lengthscale = torch.tensor(3.3)\n",
    "covar_module(x1.squeeze(), x2.squeeze()).evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d4a047",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c4d3c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.kernels import RBFKernel\n",
    "x = torch.randn(3, 2)\n",
    "RBFKernel().lengthscale = torch.tensor(3.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487c6d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test RBF input output = RBF kernel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161a48ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.kernels import RBFKernel\n",
    "\n",
    "x = torch.randn(3, 5)\n",
    "covar_module = RBFKernel()\n",
    "covar_module.lengthscale = torch.tensor(3.3)\n",
    "covar_module(x).evaluate()\n",
    "# covar_module.lengthscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3a04d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cirkit.layers.input.rbf_kernel import RBFKernelLayer\n",
    "input_la = RBFKernelLayer(num_vars=5, num_output_units=1)\n",
    "\n",
    "input_la.params = torch.nn.Parameter(torch.ones((5,1))*3.3)\n",
    "\n",
    "# input_la(x1, x2).squeeze().shape\n",
    "\n",
    "# input_la(x.unsqueeze(-1), x.unsqueeze(-1)).shape\n",
    "\n",
    "torch.prod(torch.exp(input_la(x.unsqueeze(-1), x.unsqueeze(-1)).squeeze()), dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f025e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_la = RBFKernelLayer(num_vars=20, num_output_units=1)\n",
    "\n",
    "input_la.params = torch.nn.Parameter(torch.ones((20,1))*3.3)\n",
    "\n",
    "# input_la(x1, x2).squeeze().shape\n",
    "torch.prod(input_la(x1, x1).squeeze(), dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b267ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "covar_module(x1).evaluate().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d377b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c538f29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = torch.tensor([[-0.6281], [ 0.1011], [ 0.0664]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbb7331",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cirkit.layers.input.rbf_kernel import RBFKernelLayer\n",
    "input_la = RBFKernelLayer(num_vars=2, num_output_units=1)\n",
    "\n",
    "input_la.params = torch.nn.Parameter(torch.ones((1,1))*3.3)\n",
    "\n",
    "input_la(x_2.unsqueeze(-1), x_2.unsqueeze(-1)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45183425",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_la.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a28677",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.ones((2,1))*3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4850262b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed03208",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_la(x.unsqueeze(-1), x.unsqueeze(-1)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41b397a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2.unsqueeze(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a6de01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756535e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cdist(x1, x2, p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(data_train, shuffle=True, batch_size=256)\n",
    "test_dataloader = DataLoader(data_test, shuffle=False, batch_size=256)\n",
    "optimizer = optim.SGD(pc.parameters(), lr=0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the constructed PC is not necessarily normalized, we construct the integral circuit that will compute the partition function. Note that parameters are shared and therefore there is no additional memory required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cirkit.models.functional import integrate\n",
    "pc_pf = integrate(pc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we optimize the parameters for 5 epochs by minimizing the negative log-likelohood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "for epoch_idx in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for batch, _ in train_dataloader:\n",
    "        batch = batch.to(device).unsqueeze(dim=-1)  # Add a channel dimension\n",
    "        log_score = pc(batch)\n",
    "        log_pf = pc_pf(batch)     # Compute the partition function\n",
    "        lls = log_score - log_pf  # Compute the log-likelihood\n",
    "        loss = -torch.mean(lls)   # The loss is the negative average log-likelihood\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        running_loss += loss * len(batch)\n",
    "        # Clamp the parameters to ensure they are in the intended domain\n",
    "        # This is needed if we do not use any reparametrization to ensure parameters non-negativity\n",
    "        # In our case, clamping is disable becuase we reparameterize via exponentiation (see above)\n",
    "        #for layer in model.inner_layers:\n",
    "        #    layer.clamp_params()\n",
    "    print(f\"Epoch {epoch_idx}: Average NLL: {running_loss / len(data_train):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then evaluate our model on test data by computing the average log-likelihood and bits per dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    pc.eval()\n",
    "    log_pf = pc_pf(torch.empty((), device=device))  # Compute the partition function once for testing\n",
    "    test_lls = 0.0\n",
    "    for batch, _ in test_dataloader:\n",
    "        log_score = pc(batch.to(device).unsqueeze(dim=-1))\n",
    "        lls = log_score - log_pf\n",
    "        test_lls += lls.sum().item()\n",
    "    average_ll = test_lls / len(data_test)\n",
    "    bpd = -average_ll / (num_variables * np.log(2.0))\n",
    "    print(f\"Average test LL: {average_ll:.3f}\")\n",
    "    print(f\"Bits per dimension: {bpd}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4acbcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760a9727",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf92f8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import gpytorch\n",
    "\n",
    "# from ..functions import RBFCovariance\n",
    "# from ..settings import trace_mode\n",
    "from gpytorch.kernels import Kernel\n",
    "\n",
    "\n",
    "def postprocess_rbf(dist_mat):\n",
    "    return dist_mat.div_(-2).exp_()\n",
    "\n",
    "\n",
    "class TestRBFKernel(Kernel):\n",
    "    r\"\"\"\n",
    "    Computes a covariance matrix based on the RBF (squared exponential) kernel\n",
    "    between inputs :math:`\\mathbf{x_1}` and :math:`\\mathbf{x_2}`:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "       \\begin{equation*}\n",
    "          k_{\\text{RBF}}(\\mathbf{x_1}, \\mathbf{x_2}) = \\exp \\left( -\\frac{1}{2}\n",
    "          (\\mathbf{x_1} - \\mathbf{x_2})^\\top \\Theta^{-2} (\\mathbf{x_1} - \\mathbf{x_2}) \\right)\n",
    "       \\end{equation*}\n",
    "\n",
    "    where :math:`\\Theta` is a :attr:`lengthscale` parameter.\n",
    "    See :class:`gpytorch.kernels.Kernel` for descriptions of the lengthscale options.\n",
    "\n",
    "    .. note::\n",
    "\n",
    "        This kernel does not have an `outputscale` parameter. To add a scaling parameter,\n",
    "        decorate this kernel with a :class:`gpytorch.kernels.ScaleKernel`.\n",
    "\n",
    "    Args:\n",
    "        :attr:`ard_num_dims` (int, optional):\n",
    "            Set this if you want a separate lengthscale for each\n",
    "            input dimension. It should be `d` if :attr:`x1` is a `n x d` matrix. Default: `None`\n",
    "        :attr:`batch_shape` (torch.Size, optional):\n",
    "            Set this if you want a separate lengthscale for each\n",
    "            batch of input data. It should be `b` if :attr:`x1` is a `b x n x d` tensor. Default: `torch.Size([])`.\n",
    "        :attr:`active_dims` (tuple of ints, optional):\n",
    "            Set this if you want to compute the covariance of only a few input dimensions. The ints\n",
    "            corresponds to the indices of the dimensions. Default: `None`.\n",
    "        :attr:`lengthscale_prior` (Prior, optional):\n",
    "            Set this if you want to apply a prior to the lengthscale parameter.  Default: `None`.\n",
    "        :attr:`lengthscale_constraint` (Constraint, optional):\n",
    "            Set this if you want to apply a constraint to the lengthscale parameter. Default: `Positive`.\n",
    "        :attr:`eps` (float):\n",
    "            The minimum value that the lengthscale can take (prevents divide by zero errors). Default: `1e-6`.\n",
    "\n",
    "    Attributes:\n",
    "        :attr:`lengthscale` (Tensor):\n",
    "            The lengthscale parameter. Size/shape of parameter depends on the\n",
    "            :attr:`ard_num_dims` and :attr:`batch_shape` arguments.\n",
    "\n",
    "    Example:\n",
    "        >>> x = torch.randn(10, 5)\n",
    "        >>> # Non-batch: Simple option\n",
    "        >>> covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "        >>> # Non-batch: ARD (different lengthscale for each input dimension)\n",
    "        >>> covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=5))\n",
    "        >>> covar = covar_module(x)  # Output: LazyTensor of size (10 x 10)\n",
    "        >>>\n",
    "        >>> batch_x = torch.randn(2, 10, 5)\n",
    "        >>> # Batch: Simple option\n",
    "        >>> covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "        >>> # Batch: different lengthscale for each batch\n",
    "        >>> covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(batch_shape=torch.Size([2])))\n",
    "        >>> covar = covar_module(x)  # Output: LazyTensor of size (2 x 10 x 10)\n",
    "    \"\"\"\n",
    "\n",
    "    has_lengthscale = True\n",
    "\n",
    "    def forward(self, x1, x2, diag=False, **params):\n",
    "\n",
    "        x1_ = x1.div(self.lengthscale)\n",
    "        x2_ = x2.div(self.lengthscale)\n",
    "        \n",
    "        # print (\"x1, x2\", x1_, x2_)\n",
    "        \n",
    "        return self.covar_dist(\n",
    "            x1_, x2_, square_dist=True, diag=diag, dist_postprocess_func=postprocess_rbf, postprocess=True, **params\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e9a322",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_kernel = TestRBFKernel()\n",
    "test_kernel.lengthscale = torch.tensor(3.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0114f19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_kernel.lengthscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4731298",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_kernel(x1.squeeze(),x2.squeeze()).evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8887c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d2c61d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef03dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "\n",
    "from gpytorch.constraints import Interval, Positive\n",
    "from gpytorch.priors import Prior\n",
    "from gpytorch.kernels import Kernel\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "\n",
    "class SpectralMixtureKernel(Kernel):\n",
    "    r\"\"\"\n",
    "    Computes a covariance matrix based on the Spectral Mixture Kernel\n",
    "    between inputs :math:`\\mathbf{x_1}` and :math:`\\mathbf{x_2}`.\n",
    "\n",
    "    It was proposed in `Gaussian Process Kernels for Pattern Discovery and Extrapolation`_.\n",
    "\n",
    "    .. note::\n",
    "        Unlike other kernels,\n",
    "\n",
    "            * ard_num_dims **must equal** the number of dimensions of the data.\n",
    "            * This kernel should not be combined with a :class:`gpytorch.kernels.ScaleKernel`.\n",
    "\n",
    "    :param int num_mixtures: The number of components in the mixture.\n",
    "    :param int ard_num_dims: Set this to match the dimensionality of the input.\n",
    "        It should be `d` if x1 is a `... x n x d` matrix. (Default: `1`.)\n",
    "    :param batch_shape: Set this if the data is batch of input data. It should\n",
    "        be `b_1 x ... x b_j` if x1 is a `b_1 x ... x b_j x n x d` tensor. (Default: `torch.Size([])`.)\n",
    "    :type batch_shape: torch.Size, optional\n",
    "    :param active_dims: Set this if you want to compute the covariance of only\n",
    "        a few input dimensions. The ints corresponds to the indices of the dimensions. (Default: `None`.)\n",
    "    :type active_dims: float, optional\n",
    "    :param eps: The minimum value that the lengthscale can take (prevents divide by zero errors). (Default: `1e-6`.)\n",
    "    :type eps: float, optional\n",
    "\n",
    "    :param mixture_scales_prior: A prior to set on the mixture_scales parameter\n",
    "    :type mixture_scales_prior: ~gpytorch.priors.Prior, optional\n",
    "    :param mixture_scales_constraint: A constraint to set on the mixture_scales parameter\n",
    "    :type mixture_scales_constraint: ~gpytorch.constraints.Interval, optional\n",
    "    :param mixture_means_prior: A prior to set on the mixture_means parameter\n",
    "    :type mixture_means_prior: ~gpytorch.priors.Prior, optional\n",
    "    :param mixture_means_constraint: A constraint to set on the mixture_means parameter\n",
    "    :type mixture_means_constraint: ~gpytorch.constraints.Interval, optional\n",
    "    :param mixture_weights_prior: A prior to set on the mixture_weights parameter\n",
    "    :type mixture_weights_prior: ~gpytorch.priors.Prior, optional\n",
    "    :param mixture_weights_constraint: A constraint to set on the mixture_weights parameter\n",
    "    :type mixture_weights_constraint: ~gpytorch.constraints.Interval, optional\n",
    "\n",
    "    :ivar torch.Tensor mixture_scales: The lengthscale parameter. Given\n",
    "        `k` mixture components, and `... x n x d` data, this will be of size `... x k x 1 x d`.\n",
    "    :ivar torch.Tensor mixture_means: The mixture mean parameters (`... x k x 1 x d`).\n",
    "    :ivar torch.Tensor mixture_weights: The mixture weight parameters (`... x k`).\n",
    "\n",
    "    Example:\n",
    "\n",
    "        >>> # Non-batch\n",
    "        >>> x = torch.randn(10, 5)\n",
    "        >>> covar_module = gpytorch.kernels.SpectralMixtureKernel(num_mixtures=4, ard_num_dims=5)\n",
    "        >>> covar = covar_module(x)  # Output: LazyVariable of size (10 x 10)\n",
    "        >>>\n",
    "        >>> # Batch\n",
    "        >>> batch_x = torch.randn(2, 10, 5)\n",
    "        >>> covar_module = gpytorch.kernels.SpectralMixtureKernel(num_mixtures=4, batch_size=2, ard_num_dims=5)\n",
    "        >>> covar = covar_module(x)  # Output: LazyVariable of size (10 x 10)\n",
    "\n",
    "    .. _Gaussian Process Kernels for Pattern Discovery and Extrapolation:\n",
    "        https://arxiv.org/pdf/1302.4245.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    is_stationary = True  # kernel is stationary even though it does not have a lengthscale\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_mixtures: Optional[int] = None,\n",
    "        ard_num_dims: Optional[int] = 1,\n",
    "        batch_shape: Optional[torch.Size] = torch.Size([]),\n",
    "        mixture_scales_prior: Optional[Prior] = None,\n",
    "        mixture_scales_constraint: Optional[Interval] = None,\n",
    "        mixture_means_prior: Optional[Prior] = None,\n",
    "        mixture_means_constraint: Optional[Interval] = None,\n",
    "        mixture_weights_prior: Optional[Prior] = None,\n",
    "        mixture_weights_constraint: Optional[Interval] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        if num_mixtures is None:\n",
    "            raise RuntimeError(\"num_mixtures is a required argument\")\n",
    "        if mixture_means_prior is not None or mixture_scales_prior is not None or mixture_weights_prior is not None:\n",
    "            logger.warning(\"Priors not implemented for SpectralMixtureKernel\")\n",
    "\n",
    "        # This kernel does not use the default lengthscale\n",
    "        super(SpectralMixtureKernel, self).__init__(ard_num_dims=ard_num_dims, batch_shape=batch_shape, **kwargs)\n",
    "        self.num_mixtures = num_mixtures\n",
    "\n",
    "        if mixture_scales_constraint is None:\n",
    "            mixture_scales_constraint = Positive()\n",
    "\n",
    "        if mixture_means_constraint is None:\n",
    "            mixture_means_constraint = Positive()\n",
    "\n",
    "        if mixture_weights_constraint is None:\n",
    "            mixture_weights_constraint = Positive()\n",
    "\n",
    "        self.register_parameter(\n",
    "            name=\"raw_mixture_weights\", parameter=torch.nn.Parameter(torch.zeros(*self.batch_shape, self.num_mixtures))\n",
    "        )\n",
    "        ms_shape = torch.Size([*self.batch_shape, self.num_mixtures, 1, self.ard_num_dims])\n",
    "        self.register_parameter(name=\"raw_mixture_means\", parameter=torch.nn.Parameter(torch.zeros(ms_shape)))\n",
    "        self.register_parameter(name=\"raw_mixture_scales\", parameter=torch.nn.Parameter(torch.zeros(ms_shape)))\n",
    "\n",
    "        self.register_constraint(\"raw_mixture_scales\", mixture_scales_constraint)\n",
    "        self.register_constraint(\"raw_mixture_means\", mixture_means_constraint)\n",
    "        self.register_constraint(\"raw_mixture_weights\", mixture_weights_constraint)\n",
    "\n",
    "    @property\n",
    "    def mixture_scales(self):\n",
    "        return self.raw_mixture_scales_constraint.transform(self.raw_mixture_scales)\n",
    "\n",
    "    @mixture_scales.setter\n",
    "    def mixture_scales(self, value: Union[torch.Tensor, float]):\n",
    "        self._set_mixture_scales(value)\n",
    "\n",
    "    def _set_mixture_scales(self, value: Union[torch.Tensor, float]):\n",
    "        if not torch.is_tensor(value):\n",
    "            value = torch.as_tensor(value).to(self.raw_mixture_scales)\n",
    "        self.initialize(raw_mixture_scales=self.raw_mixture_scales_constraint.inverse_transform(value))\n",
    "\n",
    "    @property\n",
    "    def mixture_means(self):\n",
    "        return self.raw_mixture_means_constraint.transform(self.raw_mixture_means)\n",
    "\n",
    "    @mixture_means.setter\n",
    "    def mixture_means(self, value: Union[torch.Tensor, float]):\n",
    "        self._set_mixture_means(value)\n",
    "\n",
    "    def _set_mixture_means(self, value: Union[torch.Tensor, float]):\n",
    "        if not torch.is_tensor(value):\n",
    "            value = torch.as_tensor(value).to(self.raw_mixture_means)\n",
    "        self.initialize(raw_mixture_means=self.raw_mixture_means_constraint.inverse_transform(value))\n",
    "\n",
    "    @property\n",
    "    def mixture_weights(self):\n",
    "        return self.raw_mixture_weights_constraint.transform(self.raw_mixture_weights)\n",
    "\n",
    "    @mixture_weights.setter\n",
    "    def mixture_weights(self, value: Union[torch.Tensor, float]):\n",
    "        self._set_mixture_weights(value)\n",
    "\n",
    "    def _set_mixture_weights(self, value: Union[torch.Tensor, float]):\n",
    "        if not torch.is_tensor(value):\n",
    "            value = torch.as_tensor(value).to(self.raw_mixture_weights)\n",
    "        self.initialize(raw_mixture_weights=self.raw_mixture_weights_constraint.inverse_transform(value))\n",
    "\n",
    "    def initialize_from_data_empspect(self, train_x: torch.Tensor, train_y: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Initialize mixture components based on the empirical spectrum of the data.\n",
    "        This will often be better than the standard initialize_from_data method, but it assumes\n",
    "        that your inputs are evenly spaced.\n",
    "\n",
    "        :param torch.Tensor train_x: Training inputs\n",
    "        :param torch.Tensor train_y: Training outputs\n",
    "        \"\"\"\n",
    "\n",
    "        import numpy as np\n",
    "        from scipy.fftpack import fft\n",
    "        from scipy.integrate import cumtrapz\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if not torch.is_tensor(train_x) or not torch.is_tensor(train_y):\n",
    "                raise RuntimeError(\"train_x and train_y should be tensors\")\n",
    "            if train_x.ndimension() == 1:\n",
    "                train_x = train_x.unsqueeze(-1)\n",
    "            if self.active_dims is not None:\n",
    "                train_x = train_x[..., self.active_dims]\n",
    "\n",
    "            # Flatten batch dimensions\n",
    "            train_x = train_x.view(-1, train_x.size(-1))\n",
    "            train_y = train_y.view(-1)\n",
    "\n",
    "            N = train_x.size(-2)\n",
    "            emp_spect = np.abs(fft(train_y.cpu().detach().numpy())) ** 2 / N\n",
    "            M = math.floor(N / 2)\n",
    "\n",
    "            freq1 = np.arange(M + 1)\n",
    "            freq2 = np.arange(-M + 1, 0)\n",
    "            freq = np.hstack((freq1, freq2)) / N\n",
    "            freq = freq[: M + 1]\n",
    "            emp_spect = emp_spect[: M + 1]\n",
    "\n",
    "            total_area = np.trapz(emp_spect, freq)\n",
    "            spec_cdf = np.hstack((np.zeros(1), cumtrapz(emp_spect, freq)))\n",
    "            spec_cdf = spec_cdf / total_area\n",
    "\n",
    "            a = np.random.rand(1000, self.ard_num_dims)\n",
    "            p, q = np.histogram(a, spec_cdf)\n",
    "            bins = np.digitize(a, q)\n",
    "            slopes = (spec_cdf[bins] - spec_cdf[bins - 1]) / (freq[bins] - freq[bins - 1])\n",
    "            intercepts = spec_cdf[bins - 1] - slopes * freq[bins - 1]\n",
    "            inv_spec = (a - intercepts) / slopes\n",
    "\n",
    "            from sklearn.mixture import GaussianMixture\n",
    "\n",
    "            GMM = GaussianMixture(n_components=self.num_mixtures, covariance_type=\"diag\").fit(inv_spec)\n",
    "            means = GMM.means_\n",
    "            varz = GMM.covariances_\n",
    "            weights = GMM.weights_\n",
    "\n",
    "            dtype = self.raw_mixture_means.dtype\n",
    "            device = self.raw_mixture_means.device\n",
    "            self.mixture_means = torch.tensor(means, dtype=dtype, device=device).unsqueeze(-2)\n",
    "            self.mixture_scales = torch.tensor(varz, dtype=dtype, device=device).unsqueeze(-2)\n",
    "            self.mixture_weights = torch.tensor(weights, dtype=dtype, device=device)\n",
    "\n",
    "\n",
    "    def initialize_from_data(self, train_x: torch.Tensor, train_y: torch.Tensor, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize mixture components based on batch statistics of the data. You should use\n",
    "        this initialization routine if your observations are not evenly spaced.\n",
    "\n",
    "        :param torch.Tensor train_x: Training inputs\n",
    "        :param torch.Tensor train_y: Training outputs\n",
    "        \"\"\"\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if not torch.is_tensor(train_x) or not torch.is_tensor(train_y):\n",
    "                raise RuntimeError(\"train_x and train_y should be tensors\")\n",
    "            if train_x.ndimension() == 1:\n",
    "                train_x = train_x.unsqueeze(-1)\n",
    "            if self.active_dims is not None:\n",
    "                train_x = train_x[..., self.active_dims]\n",
    "\n",
    "            # Compute maximum distance between points in each dimension\n",
    "            train_x_sort = train_x.sort(dim=-2)[0]\n",
    "            max_dist = train_x_sort[..., -1, :] - train_x_sort[..., 0, :]\n",
    "\n",
    "            # Compute the minimum distance between points in each dimension\n",
    "            dists = train_x_sort[..., 1:, :] - train_x_sort[..., :-1, :]\n",
    "            # We don't want the minimum distance to be zero, so fill zero values with some large number\n",
    "            dists = torch.where(dists.eq(0.0), torch.tensor(1.0e10, dtype=train_x.dtype, device=train_x.device), dists)\n",
    "            sorted_dists = dists.sort(dim=-2)[0]\n",
    "            min_dist = sorted_dists[..., 0, :]\n",
    "\n",
    "            # Reshape min_dist and max_dist to match the shape of parameters\n",
    "            # First add a singleton data dimension (-2) and a dimension for the mixture components (-3)\n",
    "            min_dist = min_dist.unsqueeze_(-2).unsqueeze_(-3)\n",
    "            max_dist = max_dist.unsqueeze_(-2).unsqueeze_(-3)\n",
    "            # Compress any dimensions in min_dist/max_dist that correspond to singletons in the SM parameters\n",
    "            dim = -3\n",
    "            while -dim <= min_dist.dim():\n",
    "                if -dim > self.raw_mixture_scales.dim():\n",
    "                    min_dist = min_dist.min(dim=dim)[0]\n",
    "                    max_dist = max_dist.max(dim=dim)[0]\n",
    "                elif self.raw_mixture_scales.size(dim) == 1:\n",
    "                    min_dist = min_dist.min(dim=dim, keepdim=True)[0]\n",
    "                    max_dist = max_dist.max(dim=dim, keepdim=True)[0]\n",
    "                    dim -= 1\n",
    "                else:\n",
    "                    dim -= 1\n",
    "\n",
    "            # Inverse of lengthscales should be drawn from truncated Gaussian | N(0, max_dist^2) |\n",
    "            self.mixture_scales = torch.randn_like(self.raw_mixture_scales).mul_(max_dist).abs_().reciprocal_()\n",
    "            # Draw means from Unif(0, 0.5 / minimum distance between two points)\n",
    "            self.mixture_means = torch.rand_like(self.raw_mixture_means).mul_(0.5).div(min_dist)\n",
    "            # Mixture weights should be roughly the stdv of the y values divided by the number of mixtures\n",
    "            self.mixture_weights = train_y.std().div(self.num_mixtures)\n",
    "\n",
    "\n",
    "    def _create_input_grid(\n",
    "        self, x1: torch.Tensor, x2: torch.Tensor, diag: bool = False, last_dim_is_batch: bool = False, **params\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        This is a helper method for creating a grid of the kernel's inputs.\n",
    "        Use this helper rather than maually creating a meshgrid.\n",
    "\n",
    "        The grid dimensions depend on the kernel's evaluation mode.\n",
    "\n",
    "        :param torch.Tensor x1: ... x n x d\n",
    "        :param torch.Tensor x2: ... x m x d (for diag mode, these must be the same inputs)\n",
    "        :param diag: Should the Kernel compute the whole kernel, or just the diag? (Default: True.)\n",
    "        :type diag: bool, optional\n",
    "        :param last_dim_is_batch: If this is true, it treats the last dimension\n",
    "            of the data as another batch dimension.  (Useful for additive\n",
    "            structure over the dimensions). (Default: False.)\n",
    "        :type last_dim_is_batch: bool, optional\n",
    "\n",
    "        :rtype: torch.Tensor, torch.Tensor\n",
    "        :return: Grid corresponding to x1 and x2. The shape depends on the kernel's mode:\n",
    "            * `full_covar`: (`... x n x 1 x d` and `... x 1 x m x d`)\n",
    "            * `full_covar` with `last_dim_is_batch=True`: (`... x k x n x 1 x 1` and `... x k x 1 x m x 1`)\n",
    "            * `diag`: (`... x n x d` and `... x n x d`)\n",
    "            * `diag` with `last_dim_is_batch=True`: (`... x k x n x 1` and `... x k x n x 1`)\n",
    "        \"\"\"\n",
    "        x1_, x2_ = x1, x2\n",
    "        if last_dim_is_batch:\n",
    "            x1_ = x1_.transpose(-1, -2).unsqueeze(-1)\n",
    "            if torch.equal(x1, x2):\n",
    "                x2_ = x1_\n",
    "            else:\n",
    "                x2_ = x2_.transpose(-1, -2).unsqueeze(-1)\n",
    "\n",
    "        if diag:\n",
    "            return x1_, x2_\n",
    "        else:\n",
    "            return x1_.unsqueeze(-2), x2_.unsqueeze(-3)\n",
    "\n",
    "    def forward(\n",
    "        self, x1: torch.Tensor, x2: torch.Tensor, diag: bool = False, last_dim_is_batch: bool = False, **params\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        n, num_dims = x1.shape[-2:]\n",
    "\n",
    "        if not num_dims == self.ard_num_dims:\n",
    "            raise RuntimeError(\n",
    "                \"The SpectralMixtureKernel expected the input to have {} dimensionality \"\n",
    "                \"(based on the ard_num_dims argument). Got {}.\".format(self.ard_num_dims, num_dims)\n",
    "            )\n",
    "\n",
    "        # Expand x1 and x2 to account for the number of mixtures\n",
    "        # Should make x1/x2 (... x k x n x d) for k mixtures\n",
    "        x1_ = x1.unsqueeze(-3)\n",
    "        x2_ = x2.unsqueeze(-3)\n",
    "        \n",
    "        # print(\"x1_\", x1_.shape)\n",
    "        # print(\"x1_\", x1_)\n",
    "        # print(\"self.mixture_means\", self.mixture_means.shape)\n",
    "        # print(\"self.mixture_means\", self.mixture_means)\n",
    "        # print(\"self.mixture_scales\", self.mixture_scales.shape)\n",
    "        # print(\"self.mixture_scales\", self.mixture_scales)\n",
    "\n",
    "        # Compute distances - scaled by appropriate parameters\n",
    "        x1_exp = x1_ * self.mixture_scales\n",
    "        x2_exp = x2_ * self.mixture_scales\n",
    "        x1_cos = x1_ * self.mixture_means\n",
    "        x2_cos = x2_ * self.mixture_means\n",
    "        \n",
    "        # print(\"x1_exp\", x1_exp)\n",
    "        # print(\"x1_cos\", x1_cos)\n",
    "        \n",
    "        # print(\"x1_exp\", x1_exp)\n",
    "\n",
    "        # Create grids\n",
    "        x1_exp_, x2_exp_ = self._create_input_grid(x1_exp, x2_exp, diag=diag, **params)\n",
    "        x1_cos_, x2_cos_ = self._create_input_grid(x1_cos, x2_cos, diag=diag, **params)\n",
    "\n",
    "        # Compute the exponential and cosine terms\n",
    "        exp_term = (x1_exp_ - x2_exp_).pow_(2).mul_(-2 * math.pi**2)\n",
    "        cos_term = (x1_cos_ - x2_cos_).mul_(2 * math.pi)\n",
    "        res = exp_term.exp_() * cos_term.cos_()\n",
    "        \n",
    "        # print(\"exp_term\", exp_term)\n",
    "        # print(\"cos_term\", cos_term)\n",
    "        \n",
    "        # print(\"exp_term\", exp_term)\n",
    "        \n",
    "        # print(\"res\", res)\n",
    "\n",
    "        # Sum over mixtures\n",
    "        mixture_weights = self.mixture_weights.view(*self.mixture_weights.shape, 1, 1)\n",
    "        if not diag:\n",
    "            mixture_weights = mixture_weights.unsqueeze(-2)\n",
    "\n",
    "        res = (res * mixture_weights).sum(-3 if diag else -4)\n",
    "\n",
    "        # Product over dimensions\n",
    "        if last_dim_is_batch:\n",
    "            # Put feature-dimension in front of data1/data2 dimensions\n",
    "            res = res.permute(*list(range(0, res.dim() - 3)), -1, -3, -2)\n",
    "        else:\n",
    "            res = res.prod(-1)\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5028f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "train_x = torch.tensor([[0.4124, 0.5949, 0.4964, 0.7655, 0.5808],[0.1466, 0.6540, 0.6794, 0.8892, 0.8310],[0.1754, 0.4290, 0.6012, 0.4222, 0.4456]])\n",
    "\n",
    "covar_module = SpectralMixtureKernel(num_mixtures=4, ard_num_dims=5)\n",
    "covar_module.mixture_weights = torch.tensor([0.0334, 0.0334, 0.0334, 0.0334])\n",
    "covar_module.mixture_means = torch.tensor([[[11.8665,  2.0900,  3.1740,  3.5268,  1.1721]],[[ 1.7340,  8.3558,  3.6868,  0.3790,  3.3036]],[[ 9.2139,  5.0617,  4.0027,  3.9273,  1.8358]],[[16.3662,  2.0223,  3.3369,  1.4315,  2.6659]]])\n",
    "covar_module.mixture_scales = torch.tensor([[[  3.4405,   7.5435,   3.9373,   2.4702,   3.0673]],[[  3.0207,   7.0498,  10.1843,  78.9824,   2.1191]],[[  4.3353,  11.9640,  18.6242,   1.8301,   7.3544]],[[  3.3139,   3.4624,  34.5551,   7.9503, 195.3149]]])\n",
    "covar_module(train_x).evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0044ea03",
   "metadata": {},
   "outputs": [],
   "source": [
    "covar_module.mixture_scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a25297",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bb3beb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cirkit1",
   "language": "python",
   "name": "cirkit1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
