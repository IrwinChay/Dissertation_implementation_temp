{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calc_value_func(theta: float) -> np.ndarray:\n",
    "    \"\"\"Calculates the value function\n",
    "\n",
    "    **YOU MUST IMPLEMENT THIS FUNCTION FOR Q1**\n",
    "\n",
    "    **DO NOT ALTER THE MDP HERE**\n",
    "\n",
    "    Useful Variables:\n",
    "    1. `self.mpd` -- Gives access to the MDP.\n",
    "    2. `self.mdp.R` -- 3D NumPy array with the rewards for each transition.\n",
    "        E.g. the reward of transition [3] -2-> [4] (going from state 3 to state 4 with action\n",
    "        2) can be accessed with `self.R[3, 2, 4]`\n",
    "        R_{t+k+1}\n",
    "    3. `self.mdp.P` -- 3D NumPy array with transition probabilities.\n",
    "        P(s', r | s, a), from s to s', action a, reward r\n",
    "        *REMEMBER*: the sum of (STATE, ACTION, :) should be 1.0 (all actions lead somewhere)\n",
    "        E.g. the transition probability of transition [3] -2-> [4] (going from state 3 to\n",
    "        state 4 with action 2) can be accessed with `self.P[3, 2, 4]`\n",
    "\n",
    "    :param theta (float): theta is the stop threshold for value iteration\n",
    "    :return (np.ndarray of float with dim (num of states)):\n",
    "        1D NumPy array with the values of each state.\n",
    "        E.g. V[3] returns the computed value for state 3\n",
    "    \"\"\"\n",
    "    ### zero inilization\n",
    "    V = np.zeros((self.state_dim))\n",
    "    while True:\n",
    "        delta = 0\n",
    "        V_new = np.zeros_like(V)\n",
    "        # for every state, for every action, for every possible subsiquent state \n",
    "        # but only legal moves\n",
    "        for state in range(self.state_dim):\n",
    "            state_action_values = np.zeros((self.state_dim, self.action_dim))\n",
    "            for action in range(self.action_dim):\n",
    "                state_act_val = 0\n",
    "                for next_state in range(self.state_dim):\n",
    "                    ### consider only legal moves\n",
    "                    # corresponding transition probability\n",
    "                    if self.mdp.P[state, action, next_state] != 0:\n",
    "                        # value in the image in the PDF\n",
    "                        state_act_val += self.mdp.P[state, action, next_state] * \\\n",
    "                        (self.mdp.R[state, action, next_state] + self.gamma *  V[next_state])  \n",
    "                # store the value (in pi(action | state)?)    \n",
    "                state_action_values[state, action] = state_act_val\n",
    "            # get the action with the max value, for this state\n",
    "            best_action_value = np.max(state_action_values[state])\n",
    "            # for all states\n",
    "            V_new[state]  = best_action_value\n",
    "            # definition of delta, for this state\n",
    "            delta = max(delta, abs(best_action_value - V[state]))\n",
    "        V = V_new    \n",
    "            \n",
    "        if delta < theta: # stop threshold for value iteration\n",
    "            ### Update V until convergence\n",
    "            break\n",
    "    \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calc_value_func_matrix(self, theta: float) -> np.ndarray:\n",
    "    V = np.zeros(self.state_dim)  # Initialize value function vector\n",
    "    while True:\n",
    "        # Compute the expected utility of taking each action in each state\n",
    "        # Reshape V to use broadcasting: (state_dim, 1) -> (state_dim, 1, 1)\n",
    "        V_reshaped = V.reshape(-1, 1, 1)\n",
    "        # Use broadcasting to compute the sum of rewards and discounted future values\n",
    "        # for all transitions, resulting in a (state_dim, action_dim, state_dim) array\n",
    "        utility = self.mdp.P * (self.mdp.R + self.gamma * V_reshaped)\n",
    "        \n",
    "        # Sum over next states to get the expected utility for each action in each state\n",
    "        # This results in a (state_dim, action_dim) array\n",
    "        action_values = np.sum(utility, axis=2)\n",
    "        \n",
    "        # Select the best action for each state (maximum expected utility)\n",
    "        V_new = np.max(action_values, axis=1)\n",
    "        \n",
    "        # Compute the maximum change in the value function across all states\n",
    "        delta = np.max(np.abs(V_new - V))\n",
    "        \n",
    "        # Update the value function\n",
    "        V = V_new\n",
    "        \n",
    "        # Check for convergence\n",
    "        if delta < theta:\n",
    "            break\n",
    "            \n",
    "    return V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Test passed: The original and matrix-optimized methods produce the same results.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Q1 1 \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class MockMDP:\n",
    "    def __init__(self, state_dim, action_dim, gamma):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        # Initialize random transition probabilities and rewards\n",
    "        self.P = np.random.rand(state_dim, action_dim, state_dim)\n",
    "        self.R = np.random.rand(state_dim, action_dim, state_dim) - 0.5  # Rewards in [-0.5, 0.5]\n",
    "        # Normalize transition probabilities\n",
    "        self.P /= self.P.sum(axis=2, keepdims=True)\n",
    "\n",
    "    def _calc_value_func(self, theta: float) -> np.ndarray:\n",
    "        V = np.zeros((self.state_dim))\n",
    "        while True:\n",
    "            delta = 0\n",
    "            V_new = np.zeros_like(V)\n",
    "            for state in range(self.state_dim):\n",
    "                state_action_values = np.zeros((self.state_dim, self.action_dim))\n",
    "                for action in range(self.action_dim):\n",
    "                    state_act_val = 0\n",
    "                    for next_state in range(self.state_dim):\n",
    "                        if self.P[state, action, next_state] != 0:\n",
    "                            state_act_val += self.P[state, action, next_state] * \\\n",
    "                            (self.R[state, action, next_state] + self.gamma * V[next_state])\n",
    "                    state_action_values[state, action] = state_act_val\n",
    "                best_action_value = np.max(state_action_values[state])\n",
    "                V_new[state] = best_action_value\n",
    "                delta = max(delta, abs(best_action_value - V[state]))\n",
    "            V = V_new\n",
    "            if delta < theta:\n",
    "                break\n",
    "        return V\n",
    "\n",
    "    def calc_value_func_matrix(self, theta: float) -> np.ndarray:\n",
    "        V = np.zeros(self.state_dim)\n",
    "        gamma = self.gamma\n",
    "        while True:\n",
    "            action_values = np.sum(self.P * (self.R + gamma * V[None, None, :]), axis=2)\n",
    "            V_new = np.max(action_values, axis=1)\n",
    "            delta = np.max(np.abs(V_new - V))\n",
    "            V = V_new\n",
    "            if delta < theta:\n",
    "                break\n",
    "        return V\n",
    "\n",
    "# Test parameters\n",
    "state_dim = 10  # Number of states\n",
    "action_dim = 4  # Number of actions\n",
    "gamma = 0.95  # Discount factor\n",
    "theta = 0.01  # Threshold for stopping the iteration\n",
    "\n",
    "# Create a mock MDP instance\n",
    "mock_mdp = MockMDP(state_dim, action_dim, gamma)\n",
    "\n",
    "# Calculate value functions using both methods\n",
    "v_original = mock_mdp._calc_value_func(theta)\n",
    "v_matrix = mock_mdp.calc_value_func_matrix(theta)\n",
    "\n",
    "# Test if the outputs are similar within a tolerance\n",
    "np.testing.assert_array_almost_equal(v_original, v_matrix, decimal=5)\n",
    "\n",
    "\"Test passed: The original and matrix-optimized methods produce the same results.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test temp\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class MockMDP:\n",
    "    def __init__(self, state_dim, action_dim, gamma):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        # Initialize random transition probabilities and rewards\n",
    "        self.P = np.random.rand(state_dim, action_dim, state_dim)\n",
    "        self.R = np.random.rand(state_dim, action_dim, state_dim) - 0.5  # Rewards in [-0.5, 0.5]\n",
    "        # Normalize transition probabilities\n",
    "        self.P /= self.P.sum(axis=2, keepdims=True)\n",
    "\n",
    "    def _calc_value_func(self, theta: float) -> np.ndarray:\n",
    "        V = np.zeros((self.state_dim))\n",
    "        return V\n",
    "\n",
    "    def calc_value_func_matrix(self, theta: float) -> np.ndarray:\n",
    "        V = np.zeros(self.state_dim)\n",
    "        return V\n",
    "\n",
    "# Test parameters\n",
    "state_dim = 10  # Number of states\n",
    "action_dim = 4  # Number of actions\n",
    "gamma = 0.95  # Discount factor\n",
    "theta = 0.01  # Threshold for stopping the iteration\n",
    "\n",
    "# Create a mock MDP instance\n",
    "mock_mdp = MockMDP(state_dim, action_dim, gamma)\n",
    "\n",
    "# Calculate value functions using both methods\n",
    "v_original = mock_mdp._calc_value_func(theta)\n",
    "v_matrix = mock_mdp.calc_value_func_matrix(theta)\n",
    "\n",
    "# Test if the outputs are similar within a tolerance\n",
    "np.testing.assert_array_almost_equal(v_original, v_matrix, decimal=5)\n",
    "\n",
    "\"Test passed: The original and matrix-optimized methods produce the same results.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy calculation test passed: Both methods produce the same results.\n"
     ]
    }
   ],
   "source": [
    "# Test Q1 2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class MockMDP:\n",
    "    def __init__(self, state_dim, action_dim, gamma):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        # Initialize random transition probabilities and rewards\n",
    "        self.P = np.random.rand(state_dim, action_dim, state_dim)\n",
    "        self.R = np.random.rand(state_dim, action_dim, state_dim) - 0.5  # Rewards in [-0.5, 0.5]\n",
    "        # Normalize transition probabilities\n",
    "        self.P /= self.P.sum(axis=2, keepdims=True)\n",
    "\n",
    "    def _calc_policy(self, V: np.ndarray) -> np.ndarray:\n",
    "        policy = np.zeros([self.state_dim, self.action_dim])\n",
    "        \n",
    "        state_action_vals = np.zeros_like(policy)\n",
    "        for state in range(self.state_dim):\n",
    "            for action in range(self.action_dim):\n",
    "                for next_state in range(self.state_dim):\n",
    "                    if self.P[state, action, next_state] != 0:\n",
    "                        state_action_vals[state][action] += self.P[state, action, next_state] * \\\n",
    "                                        (self.R[state, action, next_state] + self.gamma * V[next_state])\n",
    "            greedy_action = np.argmax(state_action_vals[state])\n",
    "            policy[state, greedy_action] = 1\n",
    "        \n",
    "        return policy\n",
    "\n",
    "    def calc_policy_vectorized(self, V: np.ndarray) -> np.ndarray:\n",
    "        action_values = np.sum(self.P * (self.R + self.gamma * V[None, None, :]), axis=2)\n",
    "        best_actions = np.argmax(action_values, axis=1)\n",
    "        \n",
    "        policy = np.zeros([self.state_dim, self.action_dim])\n",
    "        policy[np.arange(self.state_dim), best_actions] = 1.0\n",
    "        \n",
    "        return policy\n",
    "\n",
    "# Mock test for value function V\n",
    "def test_policy_calculation():\n",
    "    state_dim = 10\n",
    "    action_dim = 4\n",
    "    gamma = 0.95\n",
    "    theta = 0.01\n",
    "    \n",
    "    # Initialize mock MDP\n",
    "    mock_mdp = MockMDP(state_dim, action_dim, gamma)\n",
    "\n",
    "    # Mock value function\n",
    "    V = np.random.rand(state_dim)  # This would normally be the output of a value function calculation method\n",
    "\n",
    "    # Calculate policies using both methods\n",
    "    policy_original = mock_mdp._calc_policy(V)\n",
    "    policy_vectorized = mock_mdp.calc_policy_vectorized(V)\n",
    "\n",
    "    # Test if the policy outputs are similar within a tolerance\n",
    "    np.testing.assert_array_almost_equal(policy_original, policy_vectorized)\n",
    "    print(\"Policy calculation test passed: Both methods produce the same results.\")\n",
    "\n",
    "# Run the policy calculation test\n",
    "test_policy_calculation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop-based output: [0.07635597 0.20214211 0.26271381 0.1478511 ]\n",
      "Vectorized output: [0.07635597 0.20214211 0.26271381 0.1478511 ]\n",
      "Test passed: Loop-based and vectorized outputs are equivalent.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy evaluation test passed: Both methods produce the same results.\n"
     ]
    }
   ],
   "source": [
    "# Test Q1 3 \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class AAMockMDP:\n",
    "    def __init__(self, state_dim, action_dim, gamma, theta, P, R):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.theta = theta\n",
    "        # Initialize random transition probabilities and rewards\n",
    "        self.P = P\n",
    "        self.R = R\n",
    "        \n",
    "    \n",
    "    def policy_eval_vectorized(self, policy: np.ndarray) -> np.ndarray:\n",
    "        V = np.zeros(self.state_dim)\n",
    "        \n",
    "        while True:\n",
    "            delta = 0\n",
    "            V_prev = V.copy()\n",
    "            \n",
    "            for state in range(state_dim):\n",
    "                expected_returns = policy[state, :, None] * self.P[state] * (self.R[state] + self.gamma * V[None, :])\n",
    "                # (action_dim, state_dim)\n",
    "                V[state] = np.sum(expected_returns)  \n",
    " \n",
    "            delta = np.max(np.abs(V - V_prev))\n",
    "            if delta < self.theta:\n",
    "                break    \n",
    "        \n",
    "        return V\n",
    "    \n",
    "    def _policy_eval(self, policy: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Computes one policy evaluation step\n",
    "\n",
    "        **YOU MUST IMPLEMENT THIS FUNCTION FOR Q1**\n",
    "\n",
    "        :param policy (np.ndarray of float with dim (num of states, num of actions)):\n",
    "            output of _calc_policy()\n",
    "            A 2D NumPy array that encodes the policy.\n",
    "            It is indexed as (STATE, ACTION) where policy[STATE, ACTION] has the probability of\n",
    "            taking action 'ACTION' in state 'STATE'.\n",
    "            REMEMBER: the sum of policy[STATE, :] should always be 1.0\n",
    "            For deterministic policies the following holds for each state S:\n",
    "            policy[S, BEST_ACTION] = 1.0\n",
    "            policy[S, OTHER_ACTIONS] = 0\n",
    "        :return (np.ndarray of float with dim (num of states)): \n",
    "            A 1D NumPy array that encodes the computed value function\n",
    "            It is indexed as (State) where V[State] is the value of state 'State'\n",
    "        \"\"\"\n",
    "        # The following is adapted from Adam Jelley's code from https://gist.github.com/AdamJelley/d1e872426b0186980a15f1b6421250c2\n",
    "        V = np.zeros(self.state_dim)\n",
    "        \n",
    "        while True:\n",
    "            delta = 0\n",
    "            for state in range(self.state_dim):\n",
    "                v = 0\n",
    "                for action, act_prob in enumerate(policy[state]):\n",
    "                    for next_state, trans_prob in enumerate(self.P[state, action]):\n",
    "                        if self.P[state, action, next_state] != 0:\n",
    "                            # act_prob * value in the image in the PDF \n",
    "                            v += act_prob * trans_prob * \\\n",
    "                                    (self.R[state, action, next_state] + self.gamma *  V[next_state])      \n",
    "                # print(\"scalar v\", v)\n",
    "                delta = max(delta, abs(v - V[state]))\n",
    "                V[state] = v   \n",
    "            if delta < self.theta:\n",
    "            # if True: \n",
    "                ### Update value function until convergence\n",
    "                break    \n",
    "        \n",
    "        return V\n",
    "\n",
    "# Assuming you have filled in the _policy_eval method in MockMDP class with your provided function\n",
    "\n",
    "def test_policy_evaluation():\n",
    "    state_dim = 5\n",
    "    action_dim = 2\n",
    "    gamma = 0.99\n",
    "    theta = 0.01\n",
    "    \n",
    "    # P = np.random.rand(state_dim, action_dim, state_dim)\n",
    "    # R = np.random.rand(state_dim, action_dim, state_dim) - 0.5  # Rewards in [-0.5, 0.5]\n",
    "    # # Normalize transition probabilities\n",
    "    # P /= P.sum(axis=2, keepdims=True)\n",
    "    \n",
    "    # Initialize mock MDP\n",
    "    mock_mdp = AAMockMDP(state_dim, action_dim, gamma, theta, P, R)\n",
    "    \n",
    "    # Initialize a deterministic policy for testing\n",
    "    policy = np.zeros((state_dim, action_dim))\n",
    "    for s in range(state_dim):\n",
    "        policy[s, np.random.randint(action_dim)] = 1\n",
    "    \n",
    "    # Evaluate the policy using both methods\n",
    "    V_vectorized = mock_mdp.policy_eval_vectorized(policy)\n",
    "    V_original = mock_mdp._policy_eval(policy)\n",
    "    \n",
    "    \n",
    "    # Test if the value function outputs are similar within a tolerance\n",
    "    np.testing.assert_array_almost_equal(V_original, V_vectorized, decimal=5)\n",
    "    print(\"Policy evaluation test passed: Both methods produce the same results.\")\n",
    "\n",
    "# Run the policy evaluation test\n",
    "test_policy_evaluation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 3)\n",
      "(5, 3)\n",
      "Test passed: The original and matrix-optimized methods produce the same policies and value functions.\n"
     ]
    }
   ],
   "source": [
    "# Test Q1 4\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class MockMDP:\n",
    "    def __init__(self, state_dim, action_dim, gamma, theta):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.theta = theta\n",
    "        # Initialize random transition probabilities and rewards\n",
    "        self.P = np.random.rand(state_dim, action_dim, state_dim)\n",
    "        self.R = np.random.rand(state_dim, action_dim, state_dim) - 0.5  # Rewards in [-0.5, 0.5]\n",
    "        # Normalize transition probabilities\n",
    "        self.P /= self.P.sum(axis=2, keepdims=True)\n",
    "    \n",
    "    def _policy_eval(self, policy):\n",
    "        # Simple policy evaluation for testing (not optimized for performance)\n",
    "        V = np.zeros(self.state_dim)\n",
    "        \n",
    "        while True:\n",
    "            delta = 0\n",
    "            # for every state, for every (action and act_prob), for every possible (subsiquent state and transition probability) \n",
    "            # but only legal moves\n",
    "            for state in range(self.state_dim):\n",
    "                v = 0\n",
    "                for action, act_prob in enumerate(policy[state]):\n",
    "                    for next_state, trans_prob in enumerate(self.P[state, action]):\n",
    "                        if self.P[state, action, next_state] != 0:\n",
    "                            # act_prob * value in the image in the PDF \n",
    "                            v += act_prob * trans_prob * \\\n",
    "                                    (self.R[state, action, next_state] + self.gamma *  V[next_state])      \n",
    "                # pdb.set_trace()  \n",
    "                # same as before, for every state\n",
    "                # but set V  \n",
    "                delta = max(delta, abs(v - V[state]))\n",
    "                V[state] = v      \n",
    "            if delta < self.theta:\n",
    "                ### Update value function until convergence\n",
    "                break    \n",
    "        \n",
    "        return V\n",
    "\n",
    "    # Your _policy_improvement method adapted for the MockMDP\n",
    "    def _policy_improvement(self):\n",
    "        policy = np.zeros([self.state_dim, self.action_dim])\n",
    "        V = np.zeros([self.state_dim])\n",
    "        ### PUT YOUR CODE HERE ###\n",
    "        \n",
    "        # The following is adapted from Adam Jelley's code from https://gist.github.com/AdamJelley/d1e872426b0186980a15f1b6421250c2\n",
    "        state_action_values = policy\n",
    "        greedy_policy = np.zeros_like(policy)\n",
    "        while True:   \n",
    "            policy_stable = True\n",
    "            V = self._policy_eval(policy)\n",
    "            # for every state, for every action, for every possible subsiquent state \n",
    "            # but only legal moves\n",
    "            for state in range(self.state_dim):\n",
    "                state_action_values = np.zeros_like(policy)\n",
    "                for action in range(self.action_dim):\n",
    "                    for next_state in range(self.state_dim):\n",
    "                        ### Consider only legal transitions\n",
    "                        if self.P[state, action, next_state] != 0:\n",
    "                            # equation in PDF\n",
    "                            state_action_values[state][action] += self.P[state, action, next_state] * \\\n",
    "                                    (self.R[state, action, next_state] + self.gamma *  V[next_state]) \n",
    "                # ???\n",
    "                if np.all(policy[state] == policy[state][0]) and \\\n",
    "                   np.all(state_action_values[state] == state_action_values[state][0]):\n",
    "                    old_action = np.random.choice(np.arange(self.action_dim))\n",
    "                    new_action =  np.random.choice(np.arange(self.action_dim))\n",
    "                else: \n",
    "                    old_action = np.argmax(policy[state]) \n",
    "                    new_action = np.argmax(state_action_values[state])\n",
    "                # set to 1 or 0\n",
    "                for action in range(self.action_dim):\n",
    "                    greedy_policy[state][action] = 1 if action == new_action else 0\n",
    "                    \n",
    "                if old_action != new_action:\n",
    "                    policy_stable = False\n",
    "                \n",
    "            policy = greedy_policy\n",
    "            \n",
    "            if policy_stable and not(np.all(V == 0)):\n",
    "                ### add a condition to account that V may become [0, 0, 0] eventhough\n",
    "                ### finding optimal policy. I think this is due to np.argmax() \n",
    "                ### and random.choice()\n",
    "                break\n",
    "                \n",
    "                \n",
    "        return policy, V\n",
    "\n",
    "    # The policy_improvement_matrix method\n",
    "    def policy_improvement_matrix(self):\n",
    "        policy = np.zeros([self.state_dim, self.action_dim])\n",
    "        while True:\n",
    "            V = self._policy_eval(policy)\n",
    "            act_vals = np.sum(self.P * (self.R + self.gamma * V[None, None, :]), axis=2)\n",
    "            print(act_vals.shape)\n",
    "            new_policy = np.eye(self.action_dim)[np.argmax(act_vals, axis=1)]\n",
    "            if (new_policy == policy).all():\n",
    "                break\n",
    "            policy = new_policy\n",
    "        return policy, V\n",
    "\n",
    "# Test parameters\n",
    "state_dim = 5\n",
    "action_dim = 3\n",
    "gamma = 0.95\n",
    "theta = 0.01\n",
    "\n",
    "# Create a mock MDP instance\n",
    "mock_mdp = MockMDP(state_dim, action_dim, gamma, theta)\n",
    "\n",
    "# Calculate policies and value functions using both methods\n",
    "policy1, V1 = mock_mdp._policy_improvement()\n",
    "policy2, V2 = mock_mdp.policy_improvement_matrix()\n",
    "\n",
    "# Tests to ensure that the policies and value functions are similar\n",
    "np.testing.assert_array_almost_equal(V1, V2, decimal=5)\n",
    "np.testing.assert_array_equal(policy1, policy2)\n",
    "\n",
    "print(\"Test passed: The original and matrix-optimized methods produce the same policies and value functions.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test Q2 3\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import List, Dict\n",
    "from collections import defaultdict\n",
    "\n",
    "# Assuming the existence of the original learn function as `learn_original`\n",
    "# and the adapted function as `mc_first_visit_learn`\n",
    "\n",
    "\n",
    "def learn_original(q_table: np.ndarray, sa_counts: np.ndarray, obses: np.ndarray, actions: np.ndarray, rewards: np.ndarray, gamma: float) -> None:\n",
    "        \"\"\"Updates the Q-table based on agent experience\n",
    "\n",
    "        **YOU MUST IMPLEMENT THIS FUNCTION FOR Q2**\n",
    "\n",
    "        :param obses (List(int)): list of received observations representing environmental states\n",
    "            of trajectory (in the order they were encountered)\n",
    "        :param actions (List[int]): list of indices of applied actions in trajectory (in the\n",
    "            order they were applied)\n",
    "        :param rewards (List[float]): list of received rewards during trajectory (in the order\n",
    "            they were received)\n",
    "        :return (Dict): A dictionary containing the updated Q-value of all the updated state-action pairs\n",
    "            indexed by the state action pair.\n",
    "        \"\"\"\n",
    "        updated_values = defaultdict(lambda:0)\n",
    "        \n",
    "        ### PUT YOUR CODE HERE ###\n",
    "        ### Note to myself: this is just for one episode\n",
    "        \n",
    "        all_state_act_rwd_triple = [(obses[i], actions[i], rewards[i]) for i in range(len(obses))]\n",
    "        ## consider single (state, act) pair\n",
    "        first_state_act_episode = set([(triple[0], triple[1]) for triple in all_state_act_rwd_triple])\n",
    "\n",
    "        for (obs, act) in first_state_act_episode:\n",
    "            ## calc reward from  first visit \n",
    "            first_occurence_idx = [i for i, (fst_obs, fst_act, _) in enumerate(all_state_act_rwd_triple) if (fst_obs == obs and fst_act == act)][0]\n",
    "            # print(\"first_occurence_idx\", first_occurence_idx)\n",
    "            ## calc discounted reward and average\n",
    "            updated_values[( obs, act )] += sum([triple[2]*(gamma**i) for i, triple in enumerate(all_state_act_rwd_triple[first_occurence_idx:])])\n",
    "            \n",
    "            sa_counts[( obs, act )] += 1.0\n",
    "            ## update\n",
    "            q_table[(obs, act)] = updated_values[( obs, act )] / \\\n",
    "                                       sa_counts[( obs, act )]\n",
    "                                       \n",
    "        return q_table\n",
    "        \n",
    "        \n",
    "        # return self.q_table\n",
    "\n",
    "\n",
    "# def mc_first_visit_learn(q_table: np.ndarray, sa_counts: np.ndarray, obses: np.ndarray, actions: np.ndarray, rewards: np.ndarray, gamma: float) -> None:\n",
    "#     \"\"\"\n",
    "#     Updates the Q-table based on first-visit Monte Carlo method.\n",
    "    \n",
    "#     :param q_table: 2D NumPy array of shape (n_states, n_actions) representing the Q-values.\n",
    "#     :param sa_counts: 2D NumPy array of shape (n_states, n_actions) tracking the visit counts.\n",
    "#     :param obses: 1D NumPy array of observed states in the episode.\n",
    "#     :param actions: 1D NumPy array of actions taken in the episode.\n",
    "#     :param rewards: 1D NumPy array of received rewards in the episode.\n",
    "#     :param gamma: Discount factor.\n",
    "#     \"\"\"\n",
    "#     episode_length = len(obses)\n",
    "#     G = 0  # Initialize cumulative reward\n",
    "#     for i in reversed(range(episode_length)):\n",
    "#         G = gamma * G + rewards[i]  # Update cumulative reward with discount\n",
    "#         obs, action = obses[i], actions[i]\n",
    "        \n",
    "#         # Check if this is the first visit to (obs, action)\n",
    "#         if (obs, action) not in zip(obses[:i], actions[:i]):\n",
    "#             sa_counts[obs, action] += 1  # Update visit count\n",
    "#             # Incremental update to the average\n",
    "#             q_table[obs, action] += (G - q_table[obs, action]) / sa_counts[obs, action]\n",
    "            \n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "def mc_first_visit_learn(q_table: np.ndarray, sa_counts: np.ndarray, obses: np.ndarray, actions: np.ndarray, rewards: np.ndarray, gamma: float) -> None:\n",
    "\n",
    "    \n",
    "    updated_values = defaultdict(lambda:0)\n",
    "    \n",
    "    state_act_rwd = list(zip(obses, actions, rewards))\n",
    "    state_act = {(elem[0], elem[1]) for elem in state_act_rwd}\n",
    "\n",
    "    for (state, act) in state_act:\n",
    "        idx = [i for i, (fst_obs, fst_act, _) in enumerate(state_act_rwd) if (\n",
    "            fst_obs == state and fst_act == act)][0]\n",
    "        \n",
    "        weighted_sum = sum(triple[2] * (gamma ** i) for i, triple in enumerate(state_act_rwd[idx:]))\n",
    "        \n",
    "        updated_values[(state, act)] += weighted_sum\n",
    "        sa_counts[( state, act )] += 1.0\n",
    "        q_table[(state, act)] = updated_values[( state, act )] / sa_counts[( state, act )]\n",
    "        \n",
    "    return q_table\n",
    "\n",
    "\n",
    "    \n",
    "    # state_act_rwd = np.vstack((obses, actions, rewards)).T\n",
    "    \n",
    "    # state_act = set([(elem[0], elem[1]) for elem in state_act_rwd])\n",
    "    \n",
    "    # for (state, act) in state_act:\n",
    "    #     # reward from first visit \n",
    "    #     # np.argmax to find the first True in the boolean mask\n",
    "    #     idx = np.argmax(state_act_rwd[:, 0] == state) & (state_act_rwd[:, 1] == act)\n",
    "    #     print(\"idx\", idx)\n",
    "    #     rewards = np.array([triple[2] for triple in state_act_rwd[idx:]])\n",
    "    #     gamma_powers = gamma ** np.arange(len(rewards))\n",
    "    #     updated_values[state, act] += np.sum(np.multiply(rewards, gamma_powers))\n",
    "    #     sa_counts[( state, act )] += 1.0\n",
    "    #     q_table[(state, act)] = updated_values[( state, act )] / sa_counts[( state, act )]\n",
    "    \n",
    "\n",
    "    # # Unique state-action pairs and their first indices\n",
    "    # state_actions, first_indices = np.unique(np.stack((obses, actions), axis=-1), axis=0, return_index=True)\n",
    "    \n",
    "    # # Compute discounted rewards from the first occurrence of each state-action pair\n",
    "    # discounted_rewards = np.array([np.sum(rewards[idx:] * (gamma ** np.arange(len(rewards) - idx))) for idx in first_indices])\n",
    "    \n",
    "    # # Update Q-values\n",
    "    # for i, (state, action) in enumerate(state_actions):\n",
    "    #     updated_values[(state, action)] += discounted_rewards[i]\n",
    "    #     sa_counts[(state, action)] += 1\n",
    "    #     q_table[(state, action)] = updated_values[(state, action)] / sa_counts[(state, action)]\n",
    "    \n",
    "    # # return self.q_table\n",
    "\n",
    "\n",
    "\n",
    "def generate_episode(n_states, n_actions, episode_length):\n",
    "    obses = np.random.randint(0, n_states, size=episode_length)\n",
    "    actions = np.random.randint(0, n_actions, size=episode_length)\n",
    "    rewards = np.random.randn(episode_length)  # Normal distribution for rewards\n",
    "    return obses, actions, rewards\n",
    "\n",
    "def test_mc_update_equivalence(n_states=10, n_actions=5, episode_length=20, gamma=0.95):\n",
    "    obses, actions, rewards = generate_episode(n_states, n_actions, episode_length)\n",
    "    \n",
    "    # Initialize Q-Table and counts for original function (dictionary-based)\n",
    "    q_table_original = {}\n",
    "    sa_counts_original = {}\n",
    "    for obs in range(n_states):\n",
    "        for action in range(n_actions):\n",
    "            q_table_original[(obs, action)] = 0\n",
    "            sa_counts_original[(obs, action)] = 0\n",
    "    \n",
    "    # Initialize Q-Table and counts for adapted function (NumPy arrays)\n",
    "    q_table_adapted = np.zeros((n_states, n_actions))\n",
    "    sa_counts_adapted = np.zeros((n_states, n_actions))\n",
    "    \n",
    "    # Run original function (adapt it to process a single episode)\n",
    "    # You will need to implement this part based on the `learn` function you have\n",
    "    return_org = learn_original(q_table_adapted, sa_counts_adapted, obses, actions, rewards, gamma)\n",
    "    \n",
    "    # Run adapted function\n",
    "    return_new = mc_first_visit_learn(q_table_adapted, sa_counts_adapted, obses, actions, rewards, gamma)\n",
    "    \n",
    "    np.testing.assert_array_almost_equal(return_org, return_new, decimal=5)\n",
    "    \n",
    "    \n",
    "    # Convert adapted Q-Table and counts to dictionary format for comparison\n",
    "    # q_table_adapted_dict = {(obs, action): q_table_adapted[obs, action] for obs in range(n_states) for action in range(n_actions)}\n",
    "    # sa_counts_adapted_dict = {(obs, action): sa_counts_adapted[obs, action] for obs in range(n_states) for action in range(n_actions)}\n",
    "    \n",
    "    # # Compare the Q-tables and counts\n",
    "    # assert q_table_original == q_table_adapted_dict, \"Q-Tables do not match.\"\n",
    "    # assert sa_counts_original == sa_counts_adapted_dict, \"Counts do not match.\"\n",
    "    \n",
    "    # print(\"Test passed: Both implementations produce equivalent results.\")\n",
    "\n",
    "# Example usage\n",
    "test_mc_update_equivalence()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, dtype('float64'), dtype('float64'))"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correctly implemented numpy version\n",
    "def numpy_discounted_rewards_correct(rewards, gamma):\n",
    "    # print(np.arange(len(rewards))[::-1])\n",
    "    discounted_rewards = rewards[::-1] * np.power(gamma, np.arange(len(rewards))[::-1])\n",
    "    return np.flip(np.cumsum(discounted_rewards)).dtype\n",
    "\n",
    "def original_discounted_rewards(rewards, gamma):   \n",
    "    discounted_rewards = [rewards[t] * (gamma ** t) for t in range(len(rewards))][::-1]\n",
    "    return np.flip(np.cumsum(discounted_rewards)).dtype\n",
    "\n",
    "# Test case\n",
    "rewards = [1, 2, 3, 4]\n",
    "gamma = 0.9\n",
    "\n",
    "# Applying both methods\n",
    "original_rewards_result = original_discounted_rewards(rewards, gamma)\n",
    "numpy_rewards_result = numpy_discounted_rewards_correct(rewards, gamma)\n",
    "\n",
    "# Test to verify matching results\n",
    "test_result_correct = original_rewards_result == numpy_rewards_result\n",
    "\n",
    "test_result_correct, original_rewards_result, numpy_rewards_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>, {})"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "defaultdict(lambda:0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
